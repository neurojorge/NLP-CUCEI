{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 1. Analizar Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# 1. CARGA Y EXPLORACI√ìN INICIAL DE DATOS\n",
    "# =======================================\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Carga el dataset con manejo de errores robusto\"\"\"\n",
    "    try:\n",
    "        # Intentar diferentes encodings\n",
    "        encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "                print(f\"‚úÖ Dataset cargado exitosamente con encoding: {encoding}\")\n",
    "                return df\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "                \n",
    "        # Si ning√∫n encoding funciona, usar 'errors=replace'\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', errors='replace')\n",
    "        print(\"‚ö†Ô∏è  Dataset cargado con encoding UTF-8 y errores reemplazados\")\n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Archivo no encontrado: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al cargar el archivo: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Cargar el dataset\n",
    "file_path = r\"# === NOTE: Replace with local path ===\"\n",
    "df = load_dataset(file_path)\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFORMACI√ìN B√ÅSICA DEL DATASET\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä Dimensiones: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
    "    print(f\"üìã Columnas: {list(df.columns)}\")\n",
    "    print(\"\\nüìà Primeras 5 filas:\")\n",
    "    print(df.head())\n",
    "\n",
    "# 2. AN√ÅLISIS DE CALIDAD DE DATOS\n",
    "# ===============================\n",
    "\n",
    "def analyze_data_quality(df):\n",
    "    \"\"\"An√°lisis completo de calidad de datos\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AN√ÅLISIS DE CALIDAD DE DATOS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Informaci√≥n general\n",
    "    print(\"\\nüîç INFORMACI√ìN GENERAL:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Estad√≠sticas descriptivas\n",
    "    print(\"\\nüìä ESTAD√çSTICAS DESCRIPTIVAS:\")\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "    # Valores faltantes\n",
    "    print(\"\\n‚ùå VALORES FALTANTES:\")\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Columna': missing_data.index,\n",
    "        'Valores_Faltantes': missing_data.values,\n",
    "        'Porcentaje': missing_percent.values\n",
    "    }).sort_values('Porcentaje', ascending=False)\n",
    "    \n",
    "    print(missing_df[missing_df['Valores_Faltantes'] > 0])\n",
    "    \n",
    "    # Duplicados\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nüîÑ REGISTROS DUPLICADOS: {duplicates}\")\n",
    "    \n",
    "    # Valores √∫nicos por columna\n",
    "    print(\"\\nüéØ VALORES √öNICOS POR COLUMNA:\")\n",
    "    for col in df.columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"  {col}: {unique_count} valores √∫nicos\")\n",
    "    \n",
    "    return missing_df, duplicates\n",
    "\n",
    "# Ejecutar an√°lisis de calidad\n",
    "if df is not None:\n",
    "    missing_analysis, duplicates_count = analyze_data_quality(df)\n",
    "\n",
    "# 3. AN√ÅLISIS ESPEC√çFICO DE COLUMNAS CLAVE\n",
    "# =========================================\n",
    "\n",
    "def analyze_key_columns(df):\n",
    "    \"\"\"An√°lisis detallado de columnas importantes\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AN√ÅLISIS DE COLUMNAS CLAVE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # An√°lisis de calificaciones num√©ricas\n",
    "    if 'calificacion_numerica' in df.columns:\n",
    "        print(\"\\nüìä AN√ÅLISIS DE CALIFICACIONES:\")\n",
    "        calificaciones = df['calificacion_numerica'].dropna()\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Rango: {calificaciones.min():.2f} - {calificaciones.max():.2f}\")\n",
    "        print(f\"  ‚Ä¢ Media: {calificaciones.mean():.2f}\")\n",
    "        print(f\"  ‚Ä¢ Mediana: {calificaciones.median():.2f}\")\n",
    "        print(f\"  ‚Ä¢ Desviaci√≥n est√°ndar: {calificaciones.std():.2f}\")\n",
    "        \n",
    "        # Detectar outliers\n",
    "        Q1 = calificaciones.quantile(0.25)\n",
    "        Q3 = calificaciones.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = calificaciones[(calificaciones < Q1 - 1.5*IQR) | \n",
    "                                 (calificaciones > Q3 + 1.5*IQR)]\n",
    "        print(f\"  ‚Ä¢ Outliers detectados: {len(outliers)}\")\n",
    "        if len(outliers) > 0:\n",
    "            print(f\"    Valores: {sorted(outliers.tolist())}\")\n",
    "    \n",
    "    # An√°lisis de nombres (normalizados vs originales)\n",
    "    if 'nombre_original' in df.columns and 'nombre_normalizado' in df.columns:\n",
    "        print(\"\\nüë§ AN√ÅLISIS DE NOMBRES:\")\n",
    "        nombres_orig = df['nombre_original'].dropna().nunique()\n",
    "        nombres_norm = df['nombre_normalizado'].dropna().nunique()\n",
    "        print(f\"  ‚Ä¢ Nombres originales √∫nicos: {nombres_orig}\")\n",
    "        print(f\"  ‚Ä¢ Nombres normalizados √∫nicos: {nombres_norm}\")\n",
    "        print(f\"  ‚Ä¢ Reducci√≥n: {nombres_orig - nombres_norm} duplicados eliminados\")\n",
    "    \n",
    "    # An√°lisis de departamentos\n",
    "    if 'departamento' in df.columns:\n",
    "        print(\"\\n AN√ÅLISIS DE DEPARTAMENTOS:\")\n",
    "        dept_counts = df['departamento'].value_counts().head(10)\n",
    "        print(\"  Top 10 departamentos:\")\n",
    "        for dept, count in dept_counts.items():\n",
    "            print(f\"    {dept}: {count} registros\")\n",
    "    \n",
    "    # An√°lisis de comentarios\n",
    "    if 'comentarios' in df.columns:\n",
    "        print(\"\\n AN√ÅLISIS DE COMENTARIOS:\")\n",
    "        comentarios_validos = df['comentarios'].dropna()\n",
    "        print(f\"  ‚Ä¢ Registros con comentarios: {len(comentarios_validos)}\")\n",
    "        \n",
    "        # Extraer tags m√°s comunes\n",
    "        all_tags = []\n",
    "        for comentario in comentarios_validos:\n",
    "            if isinstance(comentario, str):\n",
    "                tags = [tag.strip() for tag in comentario.split(',')]\n",
    "                all_tags.extend(tags)\n",
    "        \n",
    "        tag_counts = Counter(all_tags)\n",
    "        print(\"  ‚Ä¢ Top 10 tags m√°s frecuentes:\")\n",
    "        for tag, count in tag_counts.most_common(10):\n",
    "            print(f\"    {tag}: {count} veces\")\n",
    "\n",
    "if df is not None:\n",
    "    analyze_key_columns(df)\n",
    "\n",
    "# 4. DETECCI√ìN Y CORRECCI√ìN DE ERRORES\n",
    "# =====================================\n",
    "\n",
    "def detect_and_fix_errors(df):\n",
    "    \"\"\"Detecta y corrige errores en el dataset\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETECCI√ìN Y CORRECCI√ìN DE ERRORES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_corrected = df.copy()\n",
    "    corrections_log = []\n",
    "    \n",
    "    # 1. Normalizar nombres de departamentos\n",
    "    if 'departamento' in df_corrected.columns:\n",
    "        print(\"\\nüîß Normalizando nombres de departamentos...\")\n",
    "        \n",
    "        # Diccionario de normalizaciones\n",
    "        dept_normalizations = {\n",
    "            # Variaciones de DIVEC\n",
    "            'divec': 'DIVEC',\n",
    "            'Divec': 'DIVEC',\n",
    "            'div. ingenierias': 'DIVEC',\n",
    "            'Div. Ingenier√≠as': 'DIVEC',\n",
    "            \n",
    "            # Variaciones de Matem√°ticas\n",
    "            'matematicas': 'Matem√°ticas',\n",
    "            'Matematicas': 'Matem√°ticas',\n",
    "            'MATEMATICAS': 'Matem√°ticas',\n",
    "            'Matem{aticas': 'Matem√°ticas',\n",
    "            \n",
    "            # Variaciones de F√≠sica\n",
    "            'fisica': 'F√≠sica',\n",
    "            'Fisica': 'F√≠sica',\n",
    "            'FISICA': 'F√≠sica',\n",
    "            'Fisicca': 'F√≠sica',\n",
    "            \n",
    "            # Variaciones de Qu√≠mica\n",
    "            'quimica': 'Qu√≠mica',\n",
    "            'Quimica': 'Qu√≠mica',\n",
    "            'QUIMICA': 'Qu√≠mica',\n",
    "            \n",
    "            # Variaciones de Computaci√≥n\n",
    "            'computacion': 'Computaci√≥n',\n",
    "            'Computacion': 'Computaci√≥n',\n",
    "            'computacion e informatica': 'Computaci√≥n e Inform√°tica',\n",
    "            \n",
    "            # Variaciones de Ingenier√≠a\n",
    "            'ingenieria': 'Ingenier√≠a',\n",
    "            'Ingenieria': 'Ingenier√≠a',\n",
    "            'ingenieria quimica': 'Ingenier√≠a Qu√≠mica',\n",
    "            'ingenieria industrial': 'Ingenier√≠a Industrial',\n",
    "            \n",
    "            # CUCEI variaciones\n",
    "            'cucei': 'CUCEI',\n",
    "            'Cucei': 'CUCEI',\n",
    "        }\n",
    "        \n",
    "        # Aplicar normalizaciones\n",
    "        for old_name, new_name in dept_normalizations.items():\n",
    "            mask = df_corrected['departamento'].str.contains(old_name, case=False, na=False)\n",
    "            count = mask.sum()\n",
    "            if count > 0:\n",
    "                df_corrected.loc[mask, 'departamento'] = new_name\n",
    "                corrections_log.append(f\"Departamento: {old_name} ‚Üí {new_name} ({count} registros)\")\n",
    "    \n",
    "    # 2. Limpiar y validar calificaciones num√©ricas\n",
    "    if 'calificacion_numerica' in df_corrected.columns:\n",
    "        print(\"\\nüîß Validando calificaciones num√©ricas...\")\n",
    "        \n",
    "        # Convertir a num√©rico, forzando errores a NaN\n",
    "        df_corrected['calificacion_numerica'] = pd.to_numeric(\n",
    "            df_corrected['calificacion_numerica'], errors='coerce'\n",
    "        )\n",
    "        \n",
    "        # Validar rango (asumiendo escala 0-10)\n",
    "        invalid_grades = df_corrected[\n",
    "            (df_corrected['calificacion_numerica'] < 0) | \n",
    "            (df_corrected['calificacion_numerica'] > 10)\n",
    "        ]['calificacion_numerica'].count()\n",
    "        \n",
    "        if invalid_grades > 0:\n",
    "            # Clipear valores fuera del rango\n",
    "            df_corrected['calificacion_numerica'] = df_corrected['calificacion_numerica'].clip(0, 10)\n",
    "            corrections_log.append(f\"Calificaciones: {invalid_grades} valores fuera de rango corregidos\")\n",
    "    \n",
    "    # 3. Limpiar comentarios\n",
    "    if 'comentarios' in df_corrected.columns:\n",
    "        print(\"\\n Limpiando comentarios...\")\n",
    "        \n",
    "        def clean_comments(comment):\n",
    "            if pd.isna(comment) or comment == '':\n",
    "                return comment\n",
    "            \n",
    "            # Convertir a string y limpiar\n",
    "            comment = str(comment)\n",
    "            # Remover espacios extra\n",
    "            comment = re.sub(r'\\s+', ' ', comment).strip()\n",
    "            # Estandarizar separadores\n",
    "            comment = re.sub(r'[,;]+', ', ', comment)\n",
    "            # Remover comas al final\n",
    "            comment = comment.rstrip(', ')\n",
    "            \n",
    "            return comment\n",
    "        \n",
    "        original_comments = df_corrected['comentarios'].copy()\n",
    "        df_corrected['comentarios'] = df_corrected['comentarios'].apply(clean_comments)\n",
    "        \n",
    "        # Contar cambios\n",
    "        changes = (original_comments != df_corrected['comentarios']).sum()\n",
    "        if changes > 0:\n",
    "            corrections_log.append(f\"Comentarios: {changes} registros limpiados\")\n",
    "    \n",
    "    # 4. Eliminar duplicados exactos\n",
    "    print(\"\\n Eliminando duplicados...\")\n",
    "    original_count = len(df_corrected)\n",
    "    df_corrected = df_corrected.drop_duplicates()\n",
    "    duplicates_removed = original_count - len(df_corrected)\n",
    "    \n",
    "    if duplicates_removed > 0:\n",
    "        corrections_log.append(f\"Duplicados: {duplicates_removed} registros eliminados\")\n",
    "    \n",
    "    # Mostrar resumen de correcciones\n",
    "    print(\"\\n RESUMEN DE CORRECCIONES:\")\n",
    "    if corrections_log:\n",
    "        for correction in corrections_log:\n",
    "            print(f\"  ‚úÖ {correction}\")\n",
    "    else:\n",
    "        print(\"    No se requirieron correcciones\")\n",
    "    \n",
    "    return df_corrected, corrections_log\n",
    "\n",
    "if df is not None:\n",
    "    df_corrected, corrections = detect_and_fix_errors(df)\n",
    "\n",
    "# 5. AN√ÅLISIS ESTAD√çSTICO AVANZADO\n",
    "# =================================\n",
    "\n",
    "def advanced_statistical_analysis(df):\n",
    "    \"\"\"An√°lisis estad√≠stico avanzado del dataset\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AN√ÅLISIS ESTAD√çSTICO AVANZADO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Crear figura con m√∫ltiples subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('An√°lisis Estad√≠stico - Dataset de Evaluaciones de Profesores', fontsize=16)\n",
    "    \n",
    "    # 1. Distribuci√≥n de calificaciones\n",
    "    if 'calificacion_numerica' in df.columns:\n",
    "        calificaciones = df['calificacion_numerica'].dropna()\n",
    "        \n",
    "        # Histograma\n",
    "        axes[0,0].hist(calificaciones, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0,0].set_title('Distribuci√≥n de Calificaciones')\n",
    "        axes[0,0].set_xlabel('Calificaci√≥n')\n",
    "        axes[0,0].set_ylabel('Frecuencia')\n",
    "        axes[0,0].axvline(calificaciones.mean(), color='red', linestyle='--', \n",
    "                         label=f'Media: {calificaciones.mean():.2f}')\n",
    "        axes[0,0].legend()\n",
    "        \n",
    "        # Box plot\n",
    "        axes[0,1].boxplot(calificaciones)\n",
    "        axes[0,1].set_title('Box Plot - Calificaciones')\n",
    "        axes[0,1].set_ylabel('Calificaci√≥n')\n",
    "    \n",
    "    # 2. Top departamentos\n",
    "    if 'departamento' in df.columns:\n",
    "        dept_counts = df['departamento'].value_counts().head(10)\n",
    "        axes[0,2].barh(range(len(dept_counts)), dept_counts.values)\n",
    "        axes[0,2].set_yticks(range(len(dept_counts)))\n",
    "        axes[0,2].set_yticklabels(dept_counts.index, fontsize=8)\n",
    "        axes[0,2].set_title('Top 10 Departamentos')\n",
    "        axes[0,2].set_xlabel('N√∫mero de Profesores')\n",
    "    \n",
    "    # 3. An√°lisis de comentarios por calificaci√≥n\n",
    "    if 'comentarios' in df.columns and 'calificacion_numerica' in df.columns:\n",
    "        # Crear categor√≠as de calificaci√≥n\n",
    "        df_temp = df.copy()\n",
    "        df_temp = df_temp.dropna(subset=['calificacion_numerica'])\n",
    "        df_temp['categoria_calif'] = pd.cut(df_temp['calificacion_numerica'], \n",
    "                                           bins=[0, 5, 7, 8.5, 10], \n",
    "                                           labels=['Baja (0-5)', 'Media (5-7)', 'Alta (7-8.5)', 'Excelente (8.5-10)'])\n",
    "        \n",
    "        categoria_counts = df_temp['categoria_calif'].value_counts()\n",
    "        axes[1,0].pie(categoria_counts.values, labels=categoria_counts.index, autopct='%1.1f%%')\n",
    "        axes[1,0].set_title('Distribuci√≥n por Categor√≠a de Calificaci√≥n')\n",
    "    \n",
    "    # 4. An√°lisis temporal (si hay fechas) o an√°lisis de completitud\n",
    "    completitud = df.count() / len(df) * 100\n",
    "    axes[1,1].bar(range(len(completitud)), completitud.values)\n",
    "    axes[1,1].set_xticks(range(len(completitud)))\n",
    "    axes[1,1].set_xticklabels(completitud.index, rotation=45, ha='right', fontsize=8)\n",
    "    axes[1,1].set_title('Completitud de Datos por Columna (%)')\n",
    "    axes[1,1].set_ylabel('Porcentaje de Completitud')\n",
    "    \n",
    "    # 5. Correlaci√≥n entre variables num√©ricas\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        im = axes[1,2].imshow(corr_matrix, cmap='coolwarm', aspect='auto')\n",
    "        axes[1,2].set_xticks(range(len(corr_matrix.columns)))\n",
    "        axes[1,2].set_yticks(range(len(corr_matrix.columns)))\n",
    "        axes[1,2].set_xticklabels(corr_matrix.columns, rotation=45, ha='right', fontsize=8)\n",
    "        axes[1,2].set_yticklabels(corr_matrix.columns, fontsize=8)\n",
    "        axes[1,2].set_title('Matriz de Correlaci√≥n')\n",
    "        plt.colorbar(im, ax=axes[1,2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "if df is not None:\n",
    "    fig = advanced_statistical_analysis(df_corrected)\n",
    "\n",
    "# 6. AN√ÅLISIS DE SENTIMIENTOS EN COMENTARIOS\n",
    "# ===========================================\n",
    "\n",
    "def analyze_sentiment_comments(df):\n",
    "    \"\"\"An√°lisis de sentimientos en comentarios\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AN√ÅLISIS DE SENTIMIENTOS EN COMENTARIOS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'comentarios' not in df.columns:\n",
    "        print(\"No hay columna de comentarios para analizar\")\n",
    "        return\n",
    "    \n",
    "    # Definir diccionarios de sentimientos\n",
    "    positive_keywords = [\n",
    "        'EXCELENTE', 'BUENA', 'BRINDA APOYO', 'INSPIRACIONAL', \n",
    "        'RESPETADO', 'C√ìMICO', 'TOMAR√çA SU CLASE OTRA VEZ'\n",
    "    ]\n",
    "    \n",
    "    negative_keywords = [\n",
    "        'CALIFICA DURO', 'DIF√çCIL', 'LARGAS', 'MUCHAS TAREAS',\n",
    "        'EX√ÅMENES SORPRESA', 'OBLIGATORIA'\n",
    "    ]\n",
    "    \n",
    "    neutral_keywords = [\n",
    "        'BARCO', 'POCOS EX√ÅMENES', 'PARTICIPACI√ìN IMPORTA', \n",
    "        'ASPECTOS DE CALIFICACI√ìN CLAROS'\n",
    "    ]\n",
    "    \n",
    "    # Analizar sentimientos\n",
    "    sentiment_scores = []\n",
    "    comentarios_validos = df['comentarios'].dropna()\n",
    "    \n",
    "    for comentario in comentarios_validos:\n",
    "        if isinstance(comentario, str):\n",
    "            positive_count = sum(1 for keyword in positive_keywords if keyword in comentario.upper())\n",
    "            negative_count = sum(1 for keyword in negative_keywords if keyword in comentario.upper())\n",
    "            neutral_count = sum(1 for keyword in neutral_keywords if keyword in comentario.upper())\n",
    "            \n",
    "            # Calcular score de sentimiento\n",
    "            total_keywords = positive_count + negative_count + neutral_count\n",
    "            if total_keywords > 0:\n",
    "                sentiment_score = (positive_count - negative_count) / total_keywords\n",
    "            else:\n",
    "                sentiment_score = 0\n",
    "            \n",
    "            sentiment_scores.append({\n",
    "                'sentimiento_score': sentiment_score,\n",
    "                'positivo': positive_count,\n",
    "                'negativo': negative_count,\n",
    "                'neutral': neutral_count\n",
    "            })\n",
    "        else:\n",
    "            sentiment_scores.append({\n",
    "                'sentimiento_score': 0,\n",
    "                'positivo': 0,\n",
    "                'negativo': 0,\n",
    "                'neutral': 0\n",
    "            })\n",
    "    \n",
    "    # Crear DataFrame de sentimientos\n",
    "    sentiment_df = pd.DataFrame(sentiment_scores)\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    print(f\"\\n ESTAD√çSTICAS DE SENTIMIENTO:\")\n",
    "    print(f\"  ‚Ä¢ Score promedio: {sentiment_df['sentimiento_score'].mean():.3f}\")\n",
    "    print(f\"  ‚Ä¢ Comentarios positivos: {(sentiment_df['sentimiento_score'] > 0.1).sum()}\")\n",
    "    print(f\"  ‚Ä¢ Comentarios negativos: {(sentiment_df['sentimiento_score'] < -0.1).sum()}\")\n",
    "    print(f\"  ‚Ä¢ Comentarios neutrales: {(abs(sentiment_df['sentimiento_score']) <= 0.1).sum()}\")\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(sentiment_df['sentimiento_score'], bins=30, alpha=0.7, color='lightgreen')\n",
    "    plt.title('Distribuci√≥n de Scores de Sentimiento')\n",
    "    plt.xlabel('Score de Sentimiento')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.axvline(0, color='red', linestyle='--', label='Neutral')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sentiment_categories = ['Positivo', 'Neutral', 'Negativo']\n",
    "    sentiment_counts = [\n",
    "        (sentiment_df['sentimiento_score'] > 0.1).sum(),\n",
    "        (abs(sentiment_df['sentimiento_score']) <= 0.1).sum(),\n",
    "        (sentiment_df['sentimiento_score'] < -0.1).sum()\n",
    "    ]\n",
    "    plt.pie(sentiment_counts, labels=sentiment_categories, autopct='%1.1f%%', \n",
    "            colors=['lightgreen', 'lightgray', 'lightcoral'])\n",
    "    plt.title('Distribuci√≥n de Sentimientos')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return sentiment_df\n",
    "\n",
    "if df is not None:\n",
    "    sentiment_analysis = analyze_sentiment_comments(df_corrected)\n",
    "\n",
    "# 7. AN√ÅLISIS AVANZADO DE PATRONES\n",
    "# =================================\n",
    "\n",
    "def advanced_pattern_analysis(df):\n",
    "    \"\"\"An√°lisis avanzado de patrones en el dataset\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AN√ÅLISIS AVANZADO DE PATRONES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. An√°lisis de profesores con m√∫ltiples registros\n",
    "    if 'nombre_normalizado' in df.columns:\n",
    "        profesor_counts = df['nombre_normalizado'].value_counts()\n",
    "        profesores_multiples = profesor_counts[profesor_counts > 1]\n",
    "        \n",
    "        print(f\"\\nüë• PROFESORES CON M√öLTIPLES REGISTROS:\")\n",
    "        print(f\"  ‚Ä¢ Total profesores √∫nicos: {len(profesor_counts)}\")\n",
    "        print(f\"  ‚Ä¢ Profesores con m√∫ltiples registros: {len(profesores_multiples)}\")\n",
    "        print(f\"  ‚Ä¢ Top 5 profesores con m√°s registros:\")\n",
    "        \n",
    "        for profesor, count in profesores_multiples.head().items():\n",
    "            print(f\"    - {profesor}: {count} registros\")\n",
    "    \n",
    "    # 2. An√°lisis de correlaci√≥n entre calificaci√≥n y sentimientos\n",
    "    if 'calificacion_numerica' in df.columns and 'comentarios' in df.columns:\n",
    "        print(f\"\\n CORRELACI√ìN CALIFICACI√ìN-COMENTARIOS:\")\n",
    "        \n",
    "        # Crear dataset temporal con an√°lisis de sentimientos\n",
    "        df_temp = df.dropna(subset=['calificacion_numerica', 'comentarios']).copy()\n",
    "        \n",
    "        # Calcular m√©tricas de comentarios\n",
    "        df_temp['num_comentarios'] = df_temp['comentarios'].str.count(',') + 1\n",
    "        df_temp['comentarios_positivos'] = df_temp['comentarios'].str.count('EXCELENTE|BUENA|APOYO|INSPIRACIONAL')\n",
    "        df_temp['comentarios_negativos'] = df_temp['comentarios'].str.count('DURO|DIF√çCIL|LARGAS|MUCHAS TAREAS')\n",
    "        \n",
    "        # Correlaciones\n",
    "        correlations = {\n",
    "            'Calificaci√≥n vs Num Comentarios': df_temp['calificacion_numerica'].corr(df_temp['num_comentarios']),\n",
    "            'Calificaci√≥n vs Comentarios Positivos': df_temp['calificacion_numerica'].corr(df_temp['comentarios_positivos']),\n",
    "            'Calificaci√≥n vs Comentarios Negativos': df_temp['calificacion_numerica'].corr(df_temp['comentarios_negativos'])\n",
    "        }\n",
    "        \n",
    "        for desc, corr in correlations.items():\n",
    "            print(f\"    {desc}: {corr:.3f}\")\n",
    "    \n",
    "    # 3. An√°lisis de distribuci√≥n por departamento\n",
    "    if 'departamento' in df.columns and 'calificacion_numerica' in df.columns:\n",
    "        print(f\"\\n AN√ÅLISIS POR DEPARTAMENTO:\")\n",
    "        \n",
    "        dept_stats = df.groupby('departamento')['calificacion_numerica'].agg(['count', 'mean', 'std']).round(2)\n",
    "        dept_stats = dept_stats.sort_values('mean', ascending=False).head(10)\n",
    "        \n",
    "        print(\"  Top 10 departamentos por calificaci√≥n promedio:\")\n",
    "        for dept, row in dept_stats.iterrows():\n",
    "            if pd.notna(row['mean']):\n",
    "                print(f\"    {dept}: {row['mean']:.2f} ¬± {row['std']:.2f} ({int(row['count'])} prof.)\")\n",
    "\n",
    "def detect_data_anomalies(df):\n",
    "    \"\"\"Detecta anomal√≠as espec√≠ficas en el dataset educativo\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETECCI√ìN DE ANOMAL√çAS ESPEC√çFICAS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    anomalies = []\n",
    "    \n",
    "    # 1. Profesores con calificaciones extremas vs comentarios\n",
    "    if 'calificacion_numerica' in df.columns and 'comentarios' in df.columns:\n",
    "        # Profesores con calificaci√≥n alta pero sin comentarios positivos\n",
    "        df_temp = df.dropna(subset=['calificacion_numerica', 'comentarios'])\n",
    "        high_grade_no_positive = df_temp[\n",
    "            (df_temp['calificacion_numerica'] >= 8.5) & \n",
    "            (~df_temp['comentarios'].str.contains('EXCELENTE|BUENA|APOYO|INSPIRACIONAL', na=False))\n",
    "        ]\n",
    "        \n",
    "        if len(high_grade_no_positive) > 0:\n",
    "            anomalies.append(f\" {len(high_grade_no_positive)} profesores con calificaci√≥n alta (‚â•8.5) pero sin comentarios claramente positivos\")\n",
    "        \n",
    "        # Profesores con calificaci√≥n baja pero comentarios positivos\n",
    "        low_grade_positive = df_temp[\n",
    "            (df_temp['calificacion_numerica'] <= 5.0) & \n",
    "            (df_temp['comentarios'].str.contains('EXCELENTE|BUENA|APOYO|INSPIRACIONAL', na=False))\n",
    "        ]\n",
    "        \n",
    "        if len(low_grade_positive) > 0:\n",
    "            anomalies.append(f\"  {len(low_grade_positive)} profesores con calificaci√≥n baja (‚â§5.0) pero con comentarios positivos\")\n",
    "    \n",
    "    # 2. Registros con informaci√≥n incompleta cr√≠tica\n",
    "    critical_missing = df[\n",
    "        df[['nombre_normalizado', 'departamento', 'calificacion_numerica']].isnull().any(axis=1)\n",
    "    ]\n",
    "    \n",
    "    if len(critical_missing) > 0:\n",
    "        anomalies.append(f\" {len(critical_missing)} registros con informaci√≥n cr√≠tica faltante (nombre, departamento o calificaci√≥n)\")\n",
    "    \n",
    "    # 3. Nombres con caracteres especiales o formato inusual\n",
    "    if 'nombre_original' in df.columns:\n",
    "        unusual_names = df[df['nombre_original'].str.contains(r'[^\\w\\s,.-]|^\\d', na=False, regex=True)]\n",
    "        if len(unusual_names) > 0:\n",
    "            anomalies.append(f\" {len(unusual_names)} nombres con formato inusual o caracteres especiales\")\n",
    "    \n",
    "    # Mostrar anomal√≠as encontradas\n",
    "    if anomalies:\n",
    "        print(\"\\n ANOMAL√çAS DETECTADAS:\")\n",
    "        for anomaly in anomalies:\n",
    "            print(f\"  {anomaly}\")\n",
    "    else:\n",
    "        print(\"\\n No se detectaron anomal√≠as significativas\")\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "def create_data_quality_dashboard(df_original, df_corrected):\n",
    "    \"\"\"Crea un dashboard visual de calidad de datos\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DASHBOARD DE CALIDAD DE DATOS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    fig.suptitle('Dashboard de Calidad - Antes y Despu√©s de Correcciones', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Comparaci√≥n de valores faltantes\n",
    "    missing_orig = df_original.isnull().sum().sort_values(ascending=False)\n",
    "    missing_corr = df_corrected.isnull().sum().sort_values(ascending=False)\n",
    "    \n",
    "    x = range(len(missing_orig))\n",
    "    axes[0,0].bar([i-0.2 for i in x], missing_orig.values, width=0.4, label='Original', alpha=0.7)\n",
    "    axes[0,0].bar([i+0.2 for i in x], missing_corr.values, width=0.4, label='Corregido', alpha=0.7)\n",
    "    axes[0,0].set_title('Valores Faltantes')\n",
    "    axes[0,0].set_xticks(x)\n",
    "    axes[0,0].set_xticklabels(missing_orig.index, rotation=45, ha='right', fontsize=8)\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].set_ylabel('Cantidad')\n",
    "    \n",
    "    # 2. Comparaci√≥n de registros totales\n",
    "    sizes_orig = [df_original.shape[0] - df_corrected.shape[0], df_corrected.shape[0]]\n",
    "    labels_orig = ['Eliminados', 'Conservados']\n",
    "    axes[0,1].pie(sizes_orig, labels=labels_orig, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0,1].set_title('Registros: Original vs Corregido')\n",
    "    \n",
    "    # 3. Distribuci√≥n de calificaciones (comparaci√≥n)\n",
    "    if 'calificacion_numerica' in df_original.columns:\n",
    "        calif_orig = df_original['calificacion_numerica'].dropna()\n",
    "        calif_corr = df_corrected['calificacion_numerica'].dropna()\n",
    "        \n",
    "        axes[0,2].hist(calif_orig, bins=20, alpha=0.5, label='Original', density=True)\n",
    "        axes[0,2].hist(calif_corr, bins=20, alpha=0.5, label='Corregido', density=True)\n",
    "        axes[0,2].set_title('Distribuci√≥n de Calificaciones')\n",
    "        axes[0,2].set_xlabel('Calificaci√≥n')\n",
    "        axes[0,2].set_ylabel('Densidad')\n",
    "        axes[0,2].legend()\n",
    "    \n",
    "    # 4. Completitud por columna\n",
    "    completitud_orig = (df_original.count() / len(df_original) * 100)\n",
    "    completitud_corr = (df_corrected.count() / len(df_corrected) * 100)\n",
    "    \n",
    "    x = range(len(completitud_orig))\n",
    "    axes[0,3].plot(x, completitud_orig.values, 'o-', label='Original', markersize=6)\n",
    "    axes[0,3].plot(x, completitud_corr.values, 's-', label='Corregido', markersize=6)\n",
    "    axes[0,3].set_title('Completitud por Columna (%)')\n",
    "    axes[0,3].set_xticks(x)\n",
    "    axes[0,3].set_xticklabels(completitud_orig.index, rotation=45, ha='right', fontsize=8)\n",
    "    axes[0,3].set_ylabel('Porcentaje')\n",
    "    axes[0,3].legend()\n",
    "    axes[0,3].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Top departamentos (corregido)\n",
    "    if 'departamento' in df_corrected.columns:\n",
    "        dept_counts = df_corrected['departamento'].value_counts().head(8)\n",
    "        axes[1,0].barh(range(len(dept_counts)), dept_counts.values)\n",
    "        axes[1,0].set_yticks(range(len(dept_counts)))\n",
    "        axes[1,0].set_yticklabels(dept_counts.index, fontsize=9)\n",
    "        axes[1,0].set_title('Top Departamentos (Datos Corregidos)')\n",
    "        axes[1,0].set_xlabel('N√∫mero de Registros')\n",
    "    \n",
    "    # 6. Distribuci√≥n de longitud de comentarios\n",
    "    if 'comentarios' in df_corrected.columns:\n",
    "        comment_lengths = df_corrected['comentarios'].dropna().str.len()\n",
    "        axes[1,1].hist(comment_lengths, bins=30, alpha=0.7, color='lightgreen')\n",
    "        axes[1,1].set_title('Distribuci√≥n de Longitud de Comentarios')\n",
    "        axes[1,1].set_xlabel('Caracteres')\n",
    "        axes[1,1].set_ylabel('Frecuencia')\n",
    "        axes[1,1].axvline(comment_lengths.mean(), color='red', linestyle='--', \n",
    "                         label=f'Media: {comment_lengths.mean():.0f}')\n",
    "        axes[1,1].legend()\n",
    "    \n",
    "    # 7. Calificaciones por departamento (top 6)\n",
    "    if 'departamento' in df_corrected.columns and 'calificacion_numerica' in df_corrected.columns:\n",
    "        top_depts = df_corrected['departamento'].value_counts().head(6).index\n",
    "        dept_data = [df_corrected[df_corrected['departamento'] == dept]['calificacion_numerica'].dropna() \n",
    "                    for dept in top_depts]\n",
    "        \n",
    "        axes[1,2].boxplot(dept_data, labels=[d[:15] + '...' if len(d) > 15 else d for d in top_depts])\n",
    "        axes[1,2].set_title('Calificaciones por Departamento (Top 6)')\n",
    "        axes[1,2].set_ylabel('Calificaci√≥n')\n",
    "        axes[1,2].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "    \n",
    "    # 8. M√©tricas de calidad general\n",
    "    quality_metrics = {\n",
    "        'Completitud\\nPromedio': (df_corrected.count().sum() / (df_corrected.shape[0] * df_corrected.shape[1])) * 100,\n",
    "        'Duplicados\\n(%)': (df_corrected.duplicated().sum() / len(df_corrected)) * 100,\n",
    "        'Registros\\nV√°lidos (%)': (len(df_corrected) / len(df_original)) * 100,\n",
    "        'Calificaci√≥n\\nPromedio': df_corrected['calificacion_numerica'].mean() if 'calificacion_numerica' in df_corrected.columns else 0\n",
    "    }\n",
    "    \n",
    "    axes[1,3].bar(quality_metrics.keys(), quality_metrics.values(), \n",
    "                  color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "    axes[1,3].set_title('M√©tricas de Calidad')\n",
    "    axes[1,3].set_ylabel('Porcentaje/Valor')\n",
    "    axes[1,3].tick_params(axis='x', rotation=45, labelsize=9)\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for i, (key, value) in enumerate(quality_metrics.items()):\n",
    "        axes[1,3].text(i, value + max(quality_metrics.values()) * 0.01, \n",
    "                      f'{value:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 8. REPORTE FINAL Y EXPORTACI√ìN\n",
    "# ===============================\n",
    "\n",
    "def generate_final_report(df_original, df_corrected, corrections, sentiment_df=None):\n",
    "    \"\"\"Genera reporte final con todas las m√©tricas y estad√≠sticas\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"REPORTE FINAL - AN√ÅLISIS Y CORRECCI√ìN DE DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìä RESUMEN EJECUTIVO:\")\n",
    "    print(f\"  ‚Ä¢ Dataset original: {df_original.shape[0]} filas √ó {df_original.shape[1]} columnas\")\n",
    "    print(f\"  ‚Ä¢ Dataset corregido: {df_corrected.shape[0]} filas √ó {df_corrected.shape[1]} columnas\")\n",
    "    print(f\"  ‚Ä¢ Registros eliminados: {df_original.shape[0] - df_corrected.shape[0]}\")\n",
    "    \n",
    "    # Calidad de datos\n",
    "    print(f\"\\nüéØ CALIDAD DE DATOS:\")\n",
    "    completitud_original = (df_original.count().sum() / (df_original.shape[0] * df_original.shape[1])) * 100\n",
    "    completitud_corregida = (df_corrected.count().sum() / (df_corrected.shape[0] * df_corrected.shape[1])) * 100\n",
    "    print(f\"  ‚Ä¢ Completitud original: {completitud_original:.1f}%\")\n",
    "    print(f\"  ‚Ä¢ Completitud corregida: {completitud_corregida:.1f}%\")\n",
    "    \n",
    "    # Estad√≠sticas de calificaciones\n",
    "    if 'calificacion_numerica' in df_corrected.columns:\n",
    "        calificaciones = df_corrected['calificacion_numerica'].dropna()\n",
    "        print(f\"\\n‚≠ê ESTAD√çSTICAS DE CALIFICACIONES:\")\n",
    "        print(f\"  ‚Ä¢ Total de evaluaciones: {len(calificaciones)}\")\n",
    "        print(f\"  ‚Ä¢ Calificaci√≥n promedio: {calificaciones.mean():.2f}\")\n",
    "        print(f\"  ‚Ä¢ Rango: {calificaciones.min():.2f} - {calificaciones.max():.2f}\")\n",
    "        print(f\"  ‚Ä¢ Profesores con calificaci√≥n ‚â• 8.0: {(calificaciones >= 8.0).sum()} ({(calificaciones >= 8.0).mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Top departamentos\n",
    "    if 'departamento' in df_corrected.columns:\n",
    "        print(f\"\\nüè¢ TOP 5 DEPARTAMENTOS:\")\n",
    "        top_depts = df_corrected['departamento'].value_counts().head()\n",
    "        for i, (dept, count) in enumerate(top_depts.items(), 1):\n",
    "            print(f\"  {i}. {dept}: {count} profesores\")\n",
    "    \n",
    "    # Correcciones aplicadas\n",
    "    print(f\"\\nüîß CORRECCIONES APLICADAS:\")\n",
    "    if corrections:\n",
    "        for correction in corrections:\n",
    "            print(f\"  ‚úÖ {correction}\")\n",
    "    else:\n",
    "        print(\"    No se requirieron correcciones\")\n",
    "    \n",
    "    # An√°lisis de sentimientos\n",
    "    if sentiment_df is not None:\n",
    "        print(f\"\\nüí≠ AN√ÅLISIS DE SENTIMIENTOS:\")\n",
    "        positive_pct = (sentiment_df['sentimiento_score'] > 0.1).mean() * 100\n",
    "        negative_pct = (sentiment_df['sentimiento_score'] < -0.1).mean() * 100\n",
    "        neutral_pct = (abs(sentiment_df['sentimiento_score']) <= 0.1).mean() * 100\n",
    "        print(f\"  ‚Ä¢ Comentarios positivos: {positive_pct:.1f}%\")\n",
    "        print(f\"  ‚Ä¢ Comentarios negativos: {negative_pct:.1f}%\")\n",
    "        print(f\"  ‚Ä¢ Comentarios neutrales: {neutral_pct:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüìã RECOMENDACIONES:\")\n",
    "    print(\"  1. Implementar validaci√≥n en tiempo real para futuras entradas\")\n",
    "    print(\"  2. Establecer est√°ndares para nombres de departamentos\")\n",
    "    print(\"  3. Mejorar la recolecci√≥n de datos para reducir valores faltantes\")\n",
    "    print(\"  4. Considerar implementar un sistema de feedback estructurado\")\n",
    "    \n",
    "    return {\n",
    "        'original_shape': df_original.shape,\n",
    "        'corrected_shape': df_corrected.shape,\n",
    "        'completitud_original': completitud_original,\n",
    "        'completitud_corregida': completitud_corregida,\n",
    "        'corrections': corrections\n",
    "    }\n",
    "\n",
    "def export_cleaned_dataset(df, output_path):\n",
    "    \"\"\"Exporta el dataset limpio\"\"\"\n",
    "    try:\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nüíæ Dataset limpio exportado a: {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error al exportar: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Ejecutar an√°lisis avanzado\n",
    "if df is not None:\n",
    "    advanced_pattern_analysis(df_corrected)\n",
    "    anomalies = detect_data_anomalies(df_corrected)\n",
    "    dashboard_fig = create_data_quality_dashboard(df, df_corrected)\n",
    "\n",
    "# 9. EXPORTACI√ìN AVANZADA CON METADATOS\n",
    "# =====================================\n",
    "\n",
    "def export_with_metadata(df_corrected, corrections, anomalies, output_dir):\n",
    "    \"\"\"Exporta el dataset con metadatos completos y reporte\"\"\"\n",
    "    \n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Crear directorio si no existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Exportar dataset limpio\n",
    "    main_file = os.path.join(output_dir, 'dataset_evaluaciones_limpio.csv')\n",
    "    df_corrected.to_csv(main_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. Crear reporte de metadatos\n",
    "    metadata_file = os.path.join(output_dir, 'reporte_limpieza.txt')\n",
    "    \n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"REPORTE DE LIMPIEZA Y CORRECCI√ìN DE DATASET\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(f\"Fecha de procesamiento: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Archivo original: paste.txt\\n\")\n",
    "        f.write(f\"Archivo limpio: dataset_evaluaciones_limpio.csv\\n\\n\")\n",
    "        \n",
    "        f.write(\"CORRECCIONES APLICADAS:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        if corrections:\n",
    "            for i, correction in enumerate(corrections, 1):\n",
    "                f.write(f\"{i}. {correction}\\n\")\n",
    "        else:\n",
    "            f.write(\"No se requirieron correcciones.\\n\")\n",
    "        \n",
    "        f.write(\"\\nANOMAL√çAS DETECTADAS:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        if anomalies:\n",
    "            for i, anomaly in enumerate(anomalies, 1):\n",
    "                f.write(f\"{i}. {anomaly}\\n\")\n",
    "        else:\n",
    "            f.write(\"No se detectaron anomal√≠as significativas.\\n\")\n",
    "        \n",
    "        # Estad√≠sticas finales\n",
    "        f.write(\"\\nESTAD√çSTICAS FINALES:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        f.write(f\"Total de registros: {len(df_corrected)}\\n\")\n",
    "        f.write(f\"Total de profesores √∫nicos: {df_corrected['nombre_normalizado'].nunique()}\\n\")\n",
    "        f.write(f\"Total de departamentos: {df_corrected['departamento'].nunique()}\\n\")\n",
    "        \n",
    "        if 'calificacion_numerica' in df_corrected.columns:\n",
    "            calificaciones = df_corrected['calificacion_numerica'].dropna()\n",
    "            f.write(f\"Calificaci√≥n promedio: {calificaciones.mean():.2f}\\n\")\n",
    "            f.write(f\"Calificaci√≥n mediana: {calificaciones.median():.2f}\\n\")\n",
    "            f.write(f\"Rango de calificaciones: {calificaciones.min():.2f} - {calificaciones.max():.2f}\\n\")\n",
    "        \n",
    "        # Completitud por columna\n",
    "        f.write(\"\\nCOMPLETITUD POR COLUMNA:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        completitud = (df_corrected.count() / len(df_corrected) * 100).sort_values(ascending=False)\n",
    "        for col, pct in completitud.items():\n",
    "            f.write(f\"{col}: {pct:.1f}%\\n\")\n",
    "    \n",
    "    # 3. Exportar muestra de datos para validaci√≥n\n",
    "    sample_file = os.path.join(output_dir, 'muestra_validacion.csv')\n",
    "    sample_df = df_corrected.sample(min(100, len(df_corrected)), random_state=42)\n",
    "    sample_df.to_csv(sample_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 4. Crear diccionario de datos\n",
    "    dict_file = os.path.join(output_dir, 'diccionario_datos.txt')\n",
    "    \n",
    "    with open(dict_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"DICCIONARIO DE DATOS - Dataset de Evaluaciones de Profesores\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        column_descriptions = {\n",
    "            'id': 'Identificador √∫nico del registro',\n",
    "            'nombre_original': 'Nombre del profesor como apareci√≥ originalmente',\n",
    "            'nombre_normalizado': 'Nombre del profesor estandarizado y limpio',\n",
    "            'departamento': 'Departamento o divisi√≥n acad√©mica del profesor',\n",
    "            'materia': 'Materia o curso impartido (si disponible)',\n",
    "            'calificacion_numerica': 'Calificaci√≥n num√©rica del profesor (escala 0-10)',\n",
    "            'comentarios': 'Comentarios categorizados sobre el profesor',\n",
    "            'resena_detallada': 'Rese√±a detallada (si disponible)',\n",
    "            'archivo_fuente': 'Archivo de origen de los datos',\n",
    "            'pagina': 'P√°gina en el archivo fuente (si aplicable)',\n",
    "            'num_fragmentos': 'N√∫mero de fragmentos de informaci√≥n',\n",
    "            'fuente': 'Fuente de los datos (dataset1, dataset2, etc.)',\n",
    "            'match_id': 'ID de coincidencia para registros relacionados',\n",
    "            'match_score': 'Score de coincidencia (si aplicable)'\n",
    "        }\n",
    "        \n",
    "        for col in df_corrected.columns:\n",
    "            desc = column_descriptions.get(col, 'Descripci√≥n no disponible')\n",
    "            dtype = str(df_corrected[col].dtype)\n",
    "            unique_vals = df_corrected[col].nunique()\n",
    "            missing_pct = (df_corrected[col].isnull().sum() / len(df_corrected)) * 100\n",
    "            \n",
    "            f.write(f\"COLUMNA: {col}\\n\")\n",
    "            f.write(f\"  Descripci√≥n: {desc}\\n\")\n",
    "            f.write(f\"  Tipo de dato: {dtype}\\n\")\n",
    "            f.write(f\"  Valores √∫nicos: {unique_vals}\\n\")\n",
    "            f.write(f\"  Valores faltantes: {missing_pct:.1f}%\\n\")\n",
    "            \n",
    "            # Mostrar algunos valores de ejemplo para columnas categ√≥ricas\n",
    "            if df_corrected[col].dtype == 'object' and unique_vals <= 20 and unique_vals > 0:\n",
    "                examples = df_corrected[col].dropna().unique()[:5]\n",
    "                f.write(f\"  Ejemplos: {', '.join(map(str, examples))}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"\\n ARCHIVOS EXPORTADOS EN: {output_dir}\")\n",
    "    print(f\"   dataset_evaluaciones_limpio.csv - Dataset principal limpio\")\n",
    "    print(f\"   reporte_limpieza.txt - Reporte detallado de correcciones\")\n",
    "    print(f\"   muestra_validacion.csv - Muestra de 100 registros para validar\")\n",
    "    print(f\"   diccionario_datos.txt - Descripci√≥n de todas las columnas\")\n",
    "    \n",
    "    return {\n",
    "        'main_file': main_file,\n",
    "        'metadata_file': metadata_file,\n",
    "        'sample_file': sample_file,\n",
    "        'dictionary_file': dict_file\n",
    "    }\n",
    "\n",
    "def create_executive_summary():\n",
    "    \"\"\"Crea un resumen ejecutivo del an√°lisis\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESUMEN EJECUTIVO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\"\"\n",
    " OBJETIVO COMPLETADO: An√°lisis integral y correcci√≥n de dataset de evaluaciones de profesores\n",
    "\n",
    " RESULTADOS CLAVE:\n",
    "  ‚Ä¢ Dataset completamente limpio y estandarizado\n",
    "  ‚Ä¢ Correcciones autom√°ticas aplicadas seg√∫n mejores pr√°cticas\n",
    "  ‚Ä¢ An√°lisis de sentimientos implementado\n",
    "  ‚Ä¢ Detecci√≥n de anomal√≠as y patrones completada\n",
    "  ‚Ä¢ Visualizaciones profesionales generadas\n",
    "\n",
    " MEJORAS IMPLEMENTADAS:\n",
    "  ‚Ä¢ Normalizaci√≥n de nombres de departamentos\n",
    "  ‚Ä¢ Validaci√≥n de calificaciones num√©ricas\n",
    "  ‚Ä¢ Limpieza de comentarios y formato\n",
    "  ‚Ä¢ Eliminaci√≥n de duplicados\n",
    "  ‚Ä¢ Detecci√≥n de inconsistencias\n",
    "\n",
    " VALOR AGREGADO:\n",
    "  ‚Ä¢ Dashboard visual de calidad de datos\n",
    "  ‚Ä¢ An√°lisis de correlaciones y patrones\n",
    "  ‚Ä¢ Metadatos completos y documentaci√≥n\n",
    "  ‚Ä¢ Archivos listos para an√°lisis posterior\n",
    "  ‚Ä¢ Recomendaciones para mejoras futuras\n",
    "\n",
    " ENTREGABLES:\n",
    "  ‚Ä¢ Dataset limpio en formato CSV\n",
    "  ‚Ä¢ Reporte de correcciones aplicadas\n",
    "  ‚Ä¢ Diccionario de datos completo\n",
    "  ‚Ä¢ Muestra para validaci√≥n manual\n",
    "  ‚Ä¢ Visualizaciones y an√°lisis estad√≠stico\n",
    "\n",
    " PR√ìXIMOS PASOS RECOMENDADOS:\n",
    "  1. Validar muestra de datos manualmente\n",
    "  2. Implementar pipeline de limpieza autom√°tica\n",
    "  3. Desarrollar modelo de an√°lisis de sentimientos espec√≠fico\n",
    "  4. Crear dashboard interactivo para monitoreo continuo\n",
    "    \"\"\")\n",
    "\n",
    "# Ejecutar exportaci√≥n y resumen final\n",
    "if df is not None:\n",
    "    output_directory = r\"# === NOTE: Replace with local path ===\"\n",
    "    exported_files = export_with_metadata(df_corrected, corrections, anomalies, output_directory)\n",
    "    create_executive_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" AN√ÅLISIS COMPLETO FINALIZADO CON √âXITO\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 2. Limpieza final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Ruta del archivo original y destino\n",
    "archivo_original = r\"# === NOTE: Replace with local path ===\"\n",
    "carpeta_destino = r\"# === NOTE: Replace with local path ===\"\n",
    "archivo_final = os.path.join(carpeta_destino, \"evaluaciones_profesores_final.csv\")\n",
    "\n",
    "# Crear carpeta destino si no existe\n",
    "os.makedirs(carpeta_destino, exist_ok=True)\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    \"\"\"Limpia y normaliza texto\"\"\"\n",
    "    if pd.isna(texto) or texto == '':\n",
    "        return ''\n",
    "    \n",
    "    texto = str(texto).strip()\n",
    "    # Remover caracteres especiales al inicio\n",
    "    texto = re.sub(r'^[‚óè‚Ä¢]\\s*', '', texto)\n",
    "    # Limpiar espacios m√∫ltiples\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    return texto\n",
    "\n",
    "def extraer_profesor_de_comentarios(comentarios):\n",
    "    \"\"\"Extrae nombre del profesor de los comentarios si est√° disponible\"\"\"\n",
    "    if pd.isna(comentarios) or comentarios == '':\n",
    "        return ''\n",
    "    \n",
    "    # Buscar patrones como \"‚óè NOMBRE: comentario\"\n",
    "    patron = r'^[‚óè‚Ä¢]\\s*([^:]+):'\n",
    "    match = re.search(patron, str(comentarios))\n",
    "    if match:\n",
    "        return limpiar_texto(match.group(1))\n",
    "    return ''\n",
    "\n",
    "def procesar_csv():\n",
    "    \"\"\"Procesa y limpia el CSV\"\"\"\n",
    "    print(\"Leyendo archivo CSV...\")\n",
    "    \n",
    "    # Leer el CSV\n",
    "    try:\n",
    "        df = pd.read_csv(archivo_original, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(archivo_original, encoding='latin-1')\n",
    "    \n",
    "    print(f\"Filas originales: {len(df)}\")\n",
    "    print(f\"Columnas encontradas: {list(df.columns)}\")\n",
    "    \n",
    "    # Crear DataFrame final con las columnas requeridas\n",
    "    df_final = pd.DataFrame()\n",
    "    \n",
    "    # PROFESOR: Usar nombre_normalizado, si est√° vac√≠o usar nombre_original\n",
    "    # Si ambos est√°n vac√≠os, extraer de comentarios\n",
    "    df_final['PROFESOR'] = df['nombre_normalizado'].fillna('')\n",
    "    mask_vacio = df_final['PROFESOR'] == ''\n",
    "    df_final.loc[mask_vacio, 'PROFESOR'] = df.loc[mask_vacio, 'nombre_original'].fillna('')\n",
    "    \n",
    "    # Si a√∫n est√° vac√≠o, extraer de comentarios\n",
    "    mask_aun_vacio = df_final['PROFESOR'] == ''\n",
    "    for idx in df_final[mask_aun_vacio].index:\n",
    "        profesor_extraido = extraer_profesor_de_comentarios(df.loc[idx, 'comentarios'])\n",
    "        if profesor_extraido:\n",
    "            df_final.loc[idx, 'PROFESOR'] = profesor_extraido\n",
    "    \n",
    "    # Limpiar nombres de profesores\n",
    "    df_final['PROFESOR'] = df_final['PROFESOR'].apply(limpiar_texto)\n",
    "    \n",
    "    # ID\n",
    "    df_final['ID'] = df['id']\n",
    "    \n",
    "    # MATERIA\n",
    "    df_final['MATERIA'] = df['materia'].fillna('').apply(limpiar_texto)\n",
    "    \n",
    "    # DEPARTAMENTO\n",
    "    df_final['DEPARTAMENTO'] = df['departamento'].fillna('').apply(limpiar_texto)\n",
    "    \n",
    "    # COMENTARIOS - Limpiar y combinar con calificaci√≥n si existe\n",
    "    df_final['COMENTARIOS'] = ''\n",
    "    for idx in df.index:\n",
    "        comentarios = limpiar_texto(df.loc[idx, 'comentarios'])\n",
    "        calificacion = df.loc[idx, 'calificacion_numerica']\n",
    "        \n",
    "        # Si hay calificaci√≥n, agregarla\n",
    "        if not pd.isna(calificacion):\n",
    "            if comentarios:\n",
    "                comentarios = f\"Calificaci√≥n: {calificacion}/10 - {comentarios}\"\n",
    "            else:\n",
    "                comentarios = f\"Calificaci√≥n: {calificacion}/10\"\n",
    "        \n",
    "        df_final.loc[idx, 'COMENTARIOS'] = comentarios\n",
    "    \n",
    "    # RESE√ëA_DETALLADA\n",
    "    df_final['RESE√ëA_DETALLADA'] = df['resena_detallada'].fillna('').apply(limpiar_texto)\n",
    "    \n",
    "    # Eliminar filas donde el profesor est√© completamente vac√≠o\n",
    "    df_final = df_final[df_final['PROFESOR'] != ''].copy()\n",
    "    \n",
    "    # Eliminar duplicados basados en profesor y materia\n",
    "    print(\"Eliminando duplicados...\")\n",
    "    df_final = df_final.drop_duplicates(subset=['PROFESOR', 'MATERIA'], keep='first')\n",
    "    \n",
    "    # Reorganizar por profesor\n",
    "    df_final = df_final.sort_values(['PROFESOR', 'MATERIA']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Filas finales: {len(df_final)}\")\n",
    "    print(f\"Profesores √∫nicos: {df_final['PROFESOR'].nunique()}\")\n",
    "    print(f\"Materias √∫nicas: {df_final['MATERIA'].nunique()}\")\n",
    "    \n",
    "    # Guardar archivo final\n",
    "    df_final.to_csv(archivo_final, index=False, encoding='utf-8')\n",
    "    print(f\"\\nArchivo guardado en: {archivo_final}\")\n",
    "    \n",
    "    # Mostrar muestra de los datos\n",
    "    print(\"\\n=== MUESTRA DE LOS DATOS FINALES ===\")\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', 50)\n",
    "    print(df_final.head(10))\n",
    "    \n",
    "    # Estad√≠sticas finales\n",
    "    print(f\"\\n=== ESTAD√çSTICAS FINALES ===\")\n",
    "    print(f\"Total de registros: {len(df_final)}\")\n",
    "    print(f\"Profesores √∫nicos: {df_final['PROFESOR'].nunique()}\")\n",
    "    print(f\"Materias √∫nicas: {df_final[df_final['MATERIA'] != '']['MATERIA'].nunique()}\")\n",
    "    print(f\"Departamentos √∫nicos: {df_final[df_final['DEPARTAMENTO'] != '']['DEPARTAMENTO'].nunique()}\")\n",
    "    print(f\"Registros con comentarios: {(df_final['COMENTARIOS'] != '').sum()}\")\n",
    "    print(f\"Registros con rese√±a detallada: {(df_final['RESE√ëA_DETALLADA'] != '').sum()}\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "def verificar_calidad_datos(df):\n",
    "    \"\"\"Verifica la calidad de los datos procesados\"\"\"\n",
    "    print(\"\\n=== VERIFICACI√ìN DE CALIDAD ===\")\n",
    "    \n",
    "    # Verificar profesores perdidos o mal formateados\n",
    "    profesores_problematicos = df[df['PROFESOR'].str.len() < 3]['PROFESOR'].unique()\n",
    "    if len(profesores_problematicos) > 0:\n",
    "        print(f\"  Profesores con nombres muy cortos: {profesores_problematicos}\")\n",
    "    \n",
    "    # Verificar registros sin informaci√≥n √∫til\n",
    "    sin_info = df[(df['COMENTARIOS'] == '') & (df['RESE√ëA_DETALLADA'] == '')]\n",
    "    print(f\"Registros sin comentarios ni rese√±a: {len(sin_info)}\")\n",
    "    \n",
    "    # Mostrar algunos ejemplos de profesores con m√°s evaluaciones\n",
    "    print(\"\\n=== PROFESORES CON M√ÅS EVALUACIONES ===\")\n",
    "    conteos = df['PROFESOR'].value_counts().head(10)\n",
    "    print(conteos)\n",
    "\n",
    "# Ejecutar el procesamiento\n",
    "if __name__ == \"__main__\":\n",
    "    df_procesado = procesar_csv()\n",
    "    verificar_calidad_datos(df_procesado)\n",
    "    print(f\"\\n Proceso completado. Archivo final guardado en:\")\n",
    "    print(f\"   {archivo_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 3. Asociaci√≥n del departamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Rutas de archivos\n",
    "ruta_evaluaciones = r\"# === NOTE: Replace with local path ===\"\n",
    "ruta_departamentos = r\"# === NOTE: Replace with local path ===\"\n",
    "ruta_salida = r\"# === NOTE: Replace with local path ===\"\n",
    "\n",
    "# Crear directorio de salida si no existe\n",
    "os.makedirs(ruta_salida, exist_ok=True)\n",
    "\n",
    "# Cargar los datasets\n",
    "print(\"Cargando datos...\")\n",
    "\n",
    "# Cargar con par√°metros m√°s robustos para manejar CSVs problem√°ticos\n",
    "try:\n",
    "    df_evaluaciones = pd.read_csv(ruta_evaluaciones, \n",
    "                                encoding='utf-8',\n",
    "                                quotechar='\"',\n",
    "                                quoting=1,  # QUOTE_ALL\n",
    "                                skipinitialspace=True,\n",
    "                                on_bad_lines='skip')\n",
    "except:\n",
    "    try:\n",
    "        # Segundo intento con encoding diferente\n",
    "        df_evaluaciones = pd.read_csv(ruta_evaluaciones, \n",
    "                                    encoding='latin-1',\n",
    "                                    quotechar='\"',\n",
    "                                    quoting=1,\n",
    "                                    skipinitialspace=True,\n",
    "                                    on_bad_lines='skip')\n",
    "    except:\n",
    "        # Tercer intento: leer l√≠nea por l√≠nea y manejar errores\n",
    "        print(\"Problemas con el CSV principal. Intentando lectura m√°s robusta...\")\n",
    "        df_evaluaciones = pd.read_csv(ruta_evaluaciones, \n",
    "                                    encoding='utf-8',\n",
    "                                    sep=',',\n",
    "                                    quotechar='\"',\n",
    "                                    doublequote=True,\n",
    "                                    skipinitialspace=True,\n",
    "                                    on_bad_lines='warn',\n",
    "                                    engine='python')\n",
    "\n",
    "try:\n",
    "    df_departamentos = pd.read_csv(ruta_departamentos, \n",
    "                                 encoding='utf-8',\n",
    "                                 sep='\\t',  # Usar tab como separador\n",
    "                                 skipinitialspace=True)\n",
    "except:\n",
    "    try:\n",
    "        df_departamentos = pd.read_csv(ruta_departamentos, \n",
    "                                     encoding='latin-1',\n",
    "                                     sep='\\t',\n",
    "                                     skipinitialspace=True)\n",
    "    except:\n",
    "        # Si a√∫n falla, separar manualmente la columna fusionada\n",
    "        df_temp = pd.read_csv(ruta_departamentos, encoding='utf-8')\n",
    "        col_name = df_temp.columns[0]\n",
    "        df_departamentos = df_temp[col_name].str.split('\\t', expand=True)\n",
    "        df_departamentos.columns = ['NOMBRE', 'DEPARTAMENTO']\n",
    "\n",
    "print(f\"Evaluaciones cargadas: {len(df_evaluaciones)} registros\")\n",
    "print(f\"Departamentos cargados: {len(df_departamentos)} registros\")\n",
    "\n",
    "# Verificar si necesitamos separar las columnas del archivo de departamentos\n",
    "if len(df_departamentos.columns) == 1 and '\\t' in df_departamentos.columns[0]:\n",
    "    print(\"Detectando formato con tab, separando columnas...\")\n",
    "    col_name = df_departamentos.columns[0]\n",
    "    df_departamentos = df_departamentos[col_name].str.split('\\t', expand=True)\n",
    "    df_departamentos.columns = ['NOMBRE', 'DEPARTAMENTO']\n",
    "    print(\"Columnas separadas correctamente\")\n",
    "\n",
    "# Inspeccionar estructura de los datos\n",
    "print(\"\\nColumnas en evaluaciones:\", list(df_evaluaciones.columns))\n",
    "print(\"Columnas en departamentos:\", list(df_departamentos.columns))\n",
    "\n",
    "# Mostrar primeras filas para entender la estructura\n",
    "print(\"\\nPrimeras filas de evaluaciones:\")\n",
    "print(df_evaluaciones.head(2))\n",
    "print(\"\\nPrimeras filas de departamentos:\")\n",
    "print(df_departamentos.head(2))\n",
    "\n",
    "# Funci√≥n para limpiar y normalizar nombres\n",
    "def limpiar_nombre(nombre):\n",
    "    if pd.isna(nombre):\n",
    "        return \"\"\n",
    "    # Convertir a string y limpiar\n",
    "    nombre = str(nombre).upper().strip()\n",
    "    # Remover caracteres especiales y espacios extra\n",
    "    nombre = re.sub(r'[^\\w\\s]', ' ', nombre)\n",
    "    nombre = ' '.join(nombre.split())\n",
    "    return nombre\n",
    "\n",
    "# Funci√≥n para crear variaciones de nombres (apellido-nombre, nombre-apellido)\n",
    "def crear_variaciones_nombre(nombre):\n",
    "    if not nombre:\n",
    "        return []\n",
    "    \n",
    "    partes = nombre.split()\n",
    "    if len(partes) < 2:\n",
    "        return [nombre]\n",
    "    \n",
    "    variaciones = [nombre]\n",
    "    \n",
    "    # Si tiene m√°s de 2 partes, crear diferentes combinaciones\n",
    "    if len(partes) >= 3:\n",
    "        # Asumiendo que los primeros son nombres y los √∫ltimos apellidos\n",
    "        mitad = len(partes) // 2\n",
    "        nombre_parte = ' '.join(partes[:mitad])\n",
    "        apellido_parte = ' '.join(partes[mitad:])\n",
    "        \n",
    "        # Agregar variaciones\n",
    "        variaciones.extend([\n",
    "            f\"{apellido_parte} {nombre_parte}\",\n",
    "            f\"{nombre_parte} {apellido_parte}\"\n",
    "        ])\n",
    "        \n",
    "        # Tambi√©n probar con solo los dos primeros y dos √∫ltimos\n",
    "        if len(partes) > 2:\n",
    "            variaciones.extend([\n",
    "                f\"{partes[-1]} {partes[0]}\",  # √∫ltimo apellido + primer nombre\n",
    "                f\"{partes[0]} {partes[-1]}\",  # primer nombre + √∫ltimo apellido\n",
    "            ])\n",
    "    \n",
    "    return list(set(variaciones))  # Remover duplicados\n",
    "\n",
    "# Limpiar nombres en ambos datasets\n",
    "print(\"Limpiando nombres...\")\n",
    "df_evaluaciones['PROFESOR_LIMPIO'] = df_evaluaciones['PROFESOR'].apply(limpiar_nombre)\n",
    "df_departamentos['NOMBRE_LIMPIO'] = df_departamentos['NOMBRE'].apply(limpiar_nombre)\n",
    "\n",
    "# Crear diccionario de mapeo departamento -> divisi√≥n\n",
    "mapeo_divisiones = {\n",
    "    'DEPTO. DE FARMACOBIOLOGIA': 'Divisi√≥n de Ciencias B√°sicas',\n",
    "    'DEPTO. DE FISICA': 'Divisi√≥n de Ciencias B√°sicas',\n",
    "    'DEPTO. DE MATEMATICAS': 'Divisi√≥n de Ciencias B√°sicas',\n",
    "    'DEPTO. DE QUIMICA': 'Divisi√≥n de Ciencias B√°sicas',\n",
    "    \n",
    "    'DEPTO. DE INGENIERIA CIVIL Y TOPOGRAFIA': 'Divisi√≥n de Ingenier√≠as',\n",
    "    'DEPTO. DE INGENIERIA INDUSTRIAL': 'Divisi√≥n de Ingenier√≠as',\n",
    "    'DEPTO. DE INGENIERIA MECANICA ELECTRICA': 'Divisi√≥n de Ingenier√≠as',\n",
    "    'DEPTO. DE INGENIERIA DE PROYECTOS': 'Divisi√≥n de Ingenier√≠as',\n",
    "    'DEPTO. DE INGENIERIA QUIMICA': 'Divisi√≥n de Ingenier√≠as',\n",
    "    'DEPTO. DE MADERA, CELULOSA Y PAPEL': 'Divisi√≥n de Ingenier√≠as',\n",
    "    \n",
    "    'DEPTO DE BIOINGENIERIA TRASLACIONAL': 'Divisi√≥n de Tecnolog√≠as para la Integraci√≥n Ciber-Humana',\n",
    "    'DEPTO. DE CIENCIAS COMPUTACIONALES': 'Divisi√≥n de Tecnolog√≠as para la Integraci√≥n Ciber-Humana',\n",
    "    'DEPTO. DE INGENIERIA ELECTRO-FOTONICA': 'Divisi√≥n de Tecnolog√≠as para la Integraci√≥n Ciber-Humana',\n",
    "    'DEPTO. DE INNOVACION BASADA EN LA INFORMACION Y EL CONOCIMIENTO': 'Divisi√≥n de Tecnolog√≠as para la Integraci√≥n Ciber-Humana'\n",
    "}\n",
    "\n",
    "# Funci√≥n para encontrar la mejor coincidencia de nombre\n",
    "def encontrar_mejor_coincidencia(nombre_profesor, lista_nombres_dept, umbral=80):\n",
    "    if not nombre_profesor:\n",
    "        return None, 0\n",
    "    \n",
    "    # Crear variaciones del nombre del profesor\n",
    "    variaciones = crear_variaciones_nombre(nombre_profesor)\n",
    "    \n",
    "    mejor_coincidencia = None\n",
    "    mejor_score = 0\n",
    "    \n",
    "    # Probar cada variaci√≥n contra todos los nombres del departamento\n",
    "    for variacion in variaciones:\n",
    "        for nombre_dept in lista_nombres_dept:\n",
    "            # Usar diferentes m√©todos de comparaci√≥n\n",
    "            scores = [\n",
    "                fuzz.ratio(variacion, nombre_dept),\n",
    "                fuzz.partial_ratio(variacion, nombre_dept),\n",
    "                fuzz.token_sort_ratio(variacion, nombre_dept),\n",
    "                fuzz.token_set_ratio(variacion, nombre_dept)\n",
    "            ]\n",
    "            \n",
    "            score_max = max(scores)\n",
    "            \n",
    "            if score_max > mejor_score and score_max >= umbral:\n",
    "                mejor_score = score_max\n",
    "                mejor_coincidencia = nombre_dept\n",
    "    \n",
    "    return mejor_coincidencia, mejor_score\n",
    "\n",
    "# Realizar la asociaci√≥n\n",
    "print(\"Realizando asociaci√≥n de nombres...\")\n",
    "lista_nombres_dept = df_departamentos['NOMBRE_LIMPIO'].tolist()\n",
    "\n",
    "resultados = []\n",
    "no_encontrados = []\n",
    "\n",
    "for idx, row in df_evaluaciones.iterrows():\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Procesando registro {idx+1}/{len(df_evaluaciones)}\")\n",
    "    \n",
    "    nombre_profesor = row['PROFESOR_LIMPIO']\n",
    "    mejor_match, score = encontrar_mejor_coincidencia(nombre_profesor, lista_nombres_dept)\n",
    "    \n",
    "    if mejor_match:\n",
    "        # Encontrar el departamento correspondiente\n",
    "        dept_info = df_departamentos[df_departamentos['NOMBRE_LIMPIO'] == mejor_match].iloc[0]\n",
    "        departamento = dept_info['DEPARTAMENTO']\n",
    "        division = mapeo_divisiones.get(departamento, 'Divisi√≥n no encontrada')\n",
    "        \n",
    "        resultados.append({\n",
    "            'match_encontrado': True,\n",
    "            'departamento': departamento,\n",
    "            'division': division,\n",
    "            'score': score\n",
    "        })\n",
    "    else:\n",
    "        no_encontrados.append(nombre_profesor)\n",
    "        resultados.append({\n",
    "            'match_encontrado': False,\n",
    "            'departamento': 'No encontrado',\n",
    "            'division': 'No encontrado',\n",
    "            'score': 0\n",
    "        })\n",
    "\n",
    "# Agregar resultados al dataframe principal\n",
    "df_resultado = df_evaluaciones.copy()\n",
    "for i, resultado in enumerate(resultados):\n",
    "    df_resultado.loc[i, 'DEPARTAMENTO'] = resultado['departamento']\n",
    "    df_resultado.loc[i, 'DIVISION'] = resultado['division']\n",
    "    df_resultado.loc[i, 'SCORE_MATCH'] = resultado['score']\n",
    "\n",
    "# Consolidar columnas de rese√±as (COMENTARIOS y RESE√ëA_DETALLADA)\n",
    "print(\"Consolidando rese√±as...\")\n",
    "def consolidar_resenas(row):\n",
    "    comentarios = str(row['COMENTARIOS']) if pd.notna(row['COMENTARIOS']) else \"\"\n",
    "    resena = str(row['RESE√ëA_DETALLADA']) if pd.notna(row['RESE√ëA_DETALLADA']) else \"\"\n",
    "    \n",
    "    # Combinar ambas, eliminando duplicados si son iguales\n",
    "    if comentarios == resena:\n",
    "        return comentarios\n",
    "    elif comentarios and resena:\n",
    "        return f\"{comentarios} {resena}\"\n",
    "    else:\n",
    "        return comentarios or resena\n",
    "\n",
    "df_resultado['RESENA_COMPLETA'] = df_resultado.apply(consolidar_resenas, axis=1)\n",
    "\n",
    "# Limpiar dataset final\n",
    "print(\"Limpiando dataset final...\")\n",
    "columnas_finales = ['PROFESOR', 'MATERIA', 'DEPARTAMENTO', 'DIVISION', 'RESENA_COMPLETA']\n",
    "df_final = df_resultado[columnas_finales].copy()\n",
    "\n",
    "# Renombrar columnas para mayor claridad\n",
    "df_final.columns = ['PROFESOR', 'MATERIA', 'DEPARTAMENTO', 'DIVISION', 'COMENTARIOS']\n",
    "\n",
    "# Guardar resultado\n",
    "archivo_salida = os.path.join(ruta_salida, \"evaluaciones_con_departamentos.csv\")\n",
    "df_final.to_csv(archivo_salida, index=False, encoding='utf-8')\n",
    "\n",
    "# Guardar tambi√©n estad√≠sticas del proceso\n",
    "estadisticas = {\n",
    "    'total_registros': len(df_evaluaciones),\n",
    "    'matches_encontrados': sum(1 for r in resultados if r['match_encontrado']),\n",
    "    'no_encontrados': len(no_encontrados),\n",
    "    'porcentaje_exito': (sum(1 for r in resultados if r['match_encontrado']) / len(df_evaluaciones)) * 100\n",
    "}\n",
    "\n",
    "# Mostrar estad√≠sticas\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESUMEN DEL PROCESO\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total de registros procesados: {estadisticas['total_registros']}\")\n",
    "print(f\"Matches encontrados: {estadisticas['matches_encontrados']}\")\n",
    "print(f\"No encontrados: {estadisticas['no_encontrados']}\")\n",
    "print(f\"Porcentaje de √©xito: {estadisticas['porcentaje_exito']:.2f}%\")\n",
    "\n",
    "# Mostrar distribuci√≥n por divisi√≥n\n",
    "print(\"\\nDISTRIBUCI√ìN POR DIVISI√ìN:\")\n",
    "print(\"-\" * 30)\n",
    "division_counts = df_final['DIVISION'].value_counts()\n",
    "for division, count in division_counts.items():\n",
    "    print(f\"{division}: {count}\")\n",
    "\n",
    "# Guardar lista de no encontrados para revisi√≥n\n",
    "if no_encontrados:\n",
    "    with open(os.path.join(ruta_salida, \"profesores_no_encontrados.txt\"), 'w', encoding='utf-8') as f:\n",
    "        f.write(\"PROFESORES NO ENCONTRADOS:\\n\")\n",
    "        f.write(\"=\"*40 + \"\\n\")\n",
    "        for nombre in set(no_encontrados):  # Usar set para evitar duplicados\n",
    "            f.write(f\"{nombre}\\n\")\n",
    "\n",
    "print(f\"\\nArchivo principal guardado en: {archivo_salida}\")\n",
    "print(f\"Directorio de salida: {ruta_salida}\")\n",
    "print(\"¬°Proceso completado exitosamente!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

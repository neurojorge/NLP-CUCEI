{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- CELDA 1: IMPORTACIONES (EST√ÅNDAR Y DE SRC) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"‚úÖ A√±adido el path ra√≠z del proyecto: {project_root}\")\n",
    "\n",
    "# --- IMPORTACIONES MODULARIZADAS (V6) ---\n",
    "try:\n",
    "    \n",
    "    from src.modeling.model import OptimizedProfessorModel, EnhancedProfesorDataset\n",
    "    from src.modeling.training import optimized_train_epoch, optimized_validate\n",
    "    from src.data_processing.utils import get_optimal_weight_iic, PESOS_DIVISIONES, get_sentiment_score\n",
    "    print(\"‚úÖ M√≥dulos 'src' (Estrategia V6 - Fusi√≥n Directa) importados exitosamente.\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERROR CR√çTICO al importar desde src/: {e}\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "print(f\"‚úÖ Librer√≠as cargadas. üî• CUDA disponible: {torch.cuda.is_available()}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- CELDA 2: CONFIGURACI√ìN, DATOS E HIPERPAR√ÅMETROS (V6 CORREGIDO) ---\n",
    "\n",
    "\n",
    "def load_data_v6(embeddings_dir, data_dir):\n",
    "    \n",
    "    \n",
    "    embed_file = \"profesores_embeddings_multilingual_robust_20250905_184407_complete.pkl\" \n",
    "    embed_path = Path(embeddings_dir) / embed_file\n",
    "    \n",
    "    \n",
    "    csv_path = Path(data_dir) / \"evaluaciones_con_departamentos.csv\"\n",
    "\n",
    "    if not embed_path.exists():\n",
    "        print(f\"‚ùå ERROR FATAL: No se encontr√≥ el archivo de Embeddings: {embed_file}\")\n",
    "        print(f\"   Por favor, copia tu .pkl funcional (de Proyect_NLP) a: {embeddings_dir}\")\n",
    "        print(f\"   (Faltando: {embed_path})\")\n",
    "        return None \n",
    "    if not csv_path.exists():\n",
    "        print(f\"‚ùå ERROR FATAL: No se encontr√≥ el CSV maestro en: {csv_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üìÇ Cargando archivo de Embeddings: {embed_file}\")\n",
    "    with open(embed_path, 'rb') as f: data_pkl = pickle.load(f)\n",
    "\n",
    "    \n",
    "    embeddings = data_pkl['embeddings']\n",
    "    data_dict = data_pkl['data']\n",
    "    print(f\"üìä Diagn√≥stico de carga (cruda): {len(embeddings)} muestras.\")\n",
    "    \n",
    "    return (\n",
    "        embeddings, data_dict.get('ratings', []), data_dict.get('departments', []),\n",
    "        data_dict.get('divisions', None), data_dict.get('original_comments', []), \n",
    "        data_dict.get('subjects', []) \n",
    "    )\n",
    "\n",
    "# --- Rutas  ---\n",
    "EMBEDDINGS_DIR = r\"src/data/Embeddings\" \n",
    "DATA_DIR = r\"src/data/raw/csv_completo\"\n",
    "MODELS_DIR = r\"src/models/checkpoints\" \n",
    "RESULTS_DIR = r\"src/models/results\" \n",
    "\n",
    "# --- C. Hiperpar√°metros  ---\n",
    "EMBEDDING_DIM = 384    \n",
    "HIDDEN_DIM = 256\n",
    "DROPOUT = 0.5           \n",
    "LEARNING_RATE = 1e-3    \n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 5 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "MODEL_NAME = f\"best_model_V6_FUSION_{timestamp}.pth\"\n",
    "RESULTS_FILE = f\"resultados_V6_FUSION_{timestamp}.json\"\n",
    "\n",
    "print(f\" Configuraci√≥n Cargada (V6 - LR Ajustado):\")\n",
    "print(f\" ‚Ä¢ Device: {device}\")\n",
    "print(f\" ‚Ä¢ Learning Rate: {LEARNING_RATE} (¬°Ajustado para MLP!)\")\n",
    "print(f\" ‚Ä¢ Dropout: {DROPOUT}\")\n",
    "\n",
    "\n",
    "# --- D. Cargar y Crear DataLoaders (V6) ---\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "try:\n",
    "    data = load_data_v6(EMBEDDINGS_DIR, DATA_DIR)\n",
    "    if data:\n",
    "        full_dataset = EnhancedProfesorDataset(*data) # Pasa los 6 args de datos al Dataset V6\n",
    "        train_size = int(0.8 * len(full_dataset))\n",
    "        val_size = len(full_dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        print(f\"\\n DataLoaders V6 creados exitosamente. Samples: {len(full_dataset)}\")\n",
    "    else:\n",
    "        print(\" Error en la carga de datos. Entrenamiento detenido.\")\n",
    "except Exception as e:\n",
    "    print(f\" ERROR durante la creaci√≥n de datasets V6: {e}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# --- CELDA 3: INICIALIZAR Y ENTRENAR (V6) ---\n",
    "\n",
    "if train_loader and val_loader:\n",
    "    # 1. Inicializar Modelo V6\n",
    "    model = OptimizedProfessorModel(\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        dropout=DROPOUT\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=2, verbose=True)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\" Modelo V6 (Fusi√≥n Directa) instanciado. Par√°metros: {total_params:,}\")\n",
    "\n",
    "    # 2. Bucle de Entrenamiento\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\" INICIANDO ENTRENAMIENTO V6 (FUSI√ìN DIRECTA) | {EPOCHS} Epochs\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_corr': [], 'val_rmse': [], 'val_sent_acc': []}\n",
    "    best_val_corr = -1.0 # Empezamos negativo\n",
    "    epochs_no_improve = 0\n",
    "    best_epoch_stats = {}\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "        \n",
    "        train_metrics = optimized_train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "        print(f\"  TRAIN | Loss: {train_metrics['total_loss']:.4f}\")\n",
    "\n",
    "        val_metrics = optimized_validate(model, val_loader, device)\n",
    "        val_loss = val_metrics['val_loss']\n",
    "        val_corr = val_metrics['correlation']\n",
    "        \n",
    "        print(f\"  VAL   | Loss: {val_loss:.4f} | Corr: {val_corr:.4f} | RMSE: {val_metrics['rmse']:.4f} | Sent Acc: {val_metrics['sentiment_accuracy']:.3f}\")\n",
    "\n",
    "        history['train_loss'].append(train_metrics['total_loss'])\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_corr'].append(val_corr)\n",
    "        history['val_rmse'].append(val_metrics['rmse'])\n",
    "        history['val_sent_acc'].append(val_metrics['sentiment_accuracy'])\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_corr > best_val_corr:\n",
    "            print(f\"   ¬°Nuevo Mejor Modelo! Correlaci√≥n mejor√≥ de {best_val_corr:.4f} a {val_corr:.4f}.\")\n",
    "            best_val_corr = val_corr\n",
    "            epochs_no_improve = 0\n",
    "            \n",
    "            os.makedirs(MODELS_DIR, exist_ok=True) # Asegurarse que la carpeta exista\n",
    "            model_save_path = os.path.join(MODELS_DIR, MODEL_NAME)\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            \n",
    "            best_epoch_stats = {\n",
    "                'epoch': epoch + 1, 'best_val_corr': float(val_corr), 'val_loss': float(val_loss),\n",
    "                'val_rmse': float(val_metrics['rmse']), 'val_sent_acc': float(val_metrics['sentiment_accuracy']),\n",
    "                'model_path': model_save_path, # Guardamos la ruta limpia\n",
    "                'hyperparameters': {'lr': LEARNING_RATE, 'dropout': DROPOUT, 'batch_size': BATCH_SIZE}\n",
    "            }\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"   Sin mejora. Paciencia: {epochs_no_improve}/{PATIENCE}. (Mejor Corr: {best_val_corr:.4f})\")\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"\\n EARLY STOPPING.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" ENTRENAMIENTO FINALIZADO\")\n",
    "    print(f\" Mejor Correlaci√≥n alcanzada: {best_epoch_stats.get('best_val_corr', -1.0):.4f} (Epoch {best_epoch_stats.get('epoch', 0)})\")\n",
    "    print(f\" Modelo guardado en: {best_epoch_stats.get('model_path', 'N/A')}\")\n",
    "\n",
    "    results_data = {'best_epoch_metrics': best_epoch_stats, 'full_history': history}\n",
    "    results_path = os.path.join(RESULTS_DIR, RESULTS_FILE)\n",
    "    try:\n",
    "        os.makedirs(RESULTS_DIR, exist_ok=True) # Asegurarse que la carpeta exista\n",
    "        with open(results_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\" Resultados completos guardados en: {results_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error al guardar resultados JSON: {e}\")\n",
    "else:\n",
    "    print(\"=\"*50)\n",
    "    print(\" ENTRENAMIENTO DETENIDO: Los DataLoaders no pudieron ser creados. Revisa la Celda 2 (Rutas de Datos).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 1. Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n y Resultados: Arquitectura V6 (Fusi√≥n Directa)\n",
    "\n",
    "Presento los resultados finales de la **Arquitectura V6 (Fusi√≥n Directa)**, una evoluci√≥n significativa sobre el sistema de pesos IIC original, aplicada al an√°lisis de rese√±as acad√©micas en **Ingenier√≠a en Inform√°tica**.\n",
    "\n",
    "Tras descubrir que los modelos anteriores (V1-V5) fallaban en generalizar (colapsando en pruebas del mundo real) debido a datos faltantes (`Materia=NaN`) y un sobreajuste severo, se dise√±√≥ una nueva arquitectura. Este modelo V6 abandona los canales de procesamiento separados y en su lugar fusiona el embedding de texto (384 dims) directamente con *features* contextuales (Peso Depto/Div) y *features* de NLP (un Score de Sentimiento calculado), creando un vector de entrada unificado de 387 dimensiones.\n",
    "\n",
    "---\n",
    "\n",
    "## Configuraci√≥n del Experimento (V6 Exitoso)\n",
    "\n",
    "- **Embeddings cargados**: `profesores_embeddings_multilingual_robust_20250905_184407_complete.pkl`\n",
    "- **Muestras procesadas**: `1466`\n",
    "- **Muestras v√°lidas (post-filtrado)**: `461`\n",
    "- **Modelo entrenado en CUDA**: ‚úÖ S√≠\n",
    "- **Par√°metros totales (V6 MLP)**: **416,516** (un modelo 49% m√°s ligero y eficiente que el V4 anterior de 815,844).\n",
    "- **Hiperpar√°metros V6**: LR = **0.001** (Corregido para MLP), Dropout = **0.5**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Estad√≠sticas del Dataset V6 (Fusi√≥n Directa)\n",
    "\n",
    "El Dataset V6 crea un vector de caracter√≠sticas √∫nico por muestra, eliminando la dependencia de datos `NaN`.\n",
    "\n",
    "- **Vector de Features**: [Embedding (384) + Peso_Depto (1) + Peso_Div (1) + Score_Sent (1)] = **387 Dims**.\n",
    "- **Rating promedio**: `7.16 ¬± 2.34`\n",
    "- **Score de Sentimiento promedio (Canal 3)**: `0.184` (Confirma que el nuevo canal de features NLP est√° activo y sesgado positivamente, coincidiendo con el rating promedio).\n",
    "\n",
    "### üîπ Distribuci√≥n de Sentimientos (Basada en Rating de 461 Muestras)\n",
    "- Negativo (<6.0): `135`\n",
    "- Neutral (6.0-7.9): `105`\n",
    "- Positivo (>=8.0): `221`\n",
    "\n",
    "### üîπ Ejemplo de an√°lisis contextual (Peso de Confianza)\n",
    "- **Departamento de Ciencias Computacionales**:\n",
    "    - Muestras: `105`\n",
    "    - Rating promedio: `6.98`\n",
    "    - Peso de Confianza (Usado en Loss): `1.00`\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Entrenamiento del Modelo V6 (vs. Modelo Anterior)\n",
    "\n",
    "La correcci√≥n de la tasa de aprendizaje (de `5e-5` a `1e-3`) permiti√≥ al modelo V6 aprender, mostrando un crecimiento estable y superando al benchmark anterior.\n",
    "\n",
    "üìå **Comparativa de Entrenamiento (Correlaci√≥n en Validaci√≥n):**\n",
    "\n",
    "| √âpoca | Modelo V4 (Benchmark 0.6571) | Modelo V6 (Nuevo Modelo 0.6700) |\n",
    "|-------|------------------------------|-------------------------------|\n",
    "| 1 | -0.1332 | 0.0777 |\n",
    "| 5 | 0.0882 | 0.1080 |\n",
    "| 10 | 0.5594 | 0.3687 |\n",
    "| 15 | **0.6571** üèÜ (Sobreajustado) | 0.5637 |\n",
    "| 20 | 0.6568 | 0.6164 |\n",
    "| 25 | (Entrenamiento detenido) | 0.6385 |\n",
    "| 30 | (Entrenamiento detenido) | **0.6700** üèÜ (Mejor Pico) |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Resultados Finales: Comparativa V4 vs V6\n",
    "\n",
    "El Modelo V6 (Fusi√≥n Directa + LR Corregido) super√≥ al modelo anterior (Gating V4/V5 + LR Incorrecto) en todas las m√©tricas clave.\n",
    "\n",
    "| M√©trica | Modelo V4 (Benchmark Antiguo) | Modelo V6 (¬°NUEVO GANADOR!) | Mejora |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Correlaci√≥n (Pico)** | 0.6571 | **0.6700** | **+ 1.96%** |\n",
    "| **Sentiment Accuracy** | 64.52% | **69.90%** | **+ 5.38%** |\n",
    "| **RMSE (Error)** | 0.2038 | **0.1973** | *Menor Error* |\n",
    "| **MSE (Error)** | 0.0415 | **0.0389** | *Menor Error* |\n",
    "\n",
    "*(Resultados V6 extra√≠dos del log de entrenamiento final y el JSON de resultados)*.\n",
    "\n",
    "## üéØ Conclusi√≥n\n",
    "\n",
    "El sistema V6 (Fusi√≥n Directa) ha demostrado ser superior a la arquitectura de pesos IIC (V4/V5). Al corregir la arquitectura para fusionar features directamente (Embeddings + Contexto + Sentimiento NLP) y ajustar los hiperpar√°metros (LR `0.001`), el modelo V6 alcanz√≥ una correlaci√≥n de **0.6700** y un **69.9%** de precisi√≥n en sentimiento.\n",
    "\n",
    "Esto representa una mejora medible sobre el benchmark anterior (0.6571). Sin embargo, las pruebas de inferencia en el mundo real demuestran que el modelo a√∫n sufre de **sobreajuste severo** debido al dataset extremadamente peque√±o (461 muestras).\n",
    "\n",
    "La arquitectura V6 es la correcta, pero requiere datos de entrenamiento adicionales (como los propuestos en la \"Parte 3: Scraping de Facebook\") para generalizar y ser funcional en producci√≥n."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 1.   Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "ruta_json = \"# === NOTE: Replace with local path ===\"\n",
    "\n",
    "\n",
    "ruta_csv = \"# === NOTE: Replace with local path ===\"\n",
    "\n",
    "def limpiar_para_url(texto):\n",
    "    texto = texto.strip().upper()\n",
    "    # Cambiar espacios por guiones\n",
    "    texto = re.sub(r'\\s+', '-', texto)\n",
    "    # Quitar caracteres que no sean letras, n√∫meros o guiones\n",
    "    texto = re.sub(r'[^A-Z0-9\\-]', '', texto)\n",
    "    return texto\n",
    "\n",
    "def construir_url(profesor):\n",
    "    nombre_url = f\"{limpiar_para_url(profesor['n'])}-{limpiar_para_url(profesor['a'])}_{profesor['i']}\"\n",
    "    url = f\"https://www.misprofesores.com/profesores/{nombre_url}\"\n",
    "    return url\n",
    "\n",
    "def extraer_rese√±as(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = soup.find_all('span', class_='tag-box-choosetags')\n",
    "    rese√±as = []\n",
    "    for tag in tags:\n",
    "        texto = tag.get_text(strip=True)\n",
    "        texto_limpio = texto.rsplit('(', 1)[0].strip()\n",
    "        rese√±as.append(texto_limpio)\n",
    "    return rese√±as\n",
    "\n",
    "def main():\n",
    "    # Leer JSON\n",
    "    with open(ruta_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        profesores = json.load(f)\n",
    "\n",
    "    with open(ruta_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        campos = ['Nombre Completo', 'Departamento', 'Calificaci√≥n', 'Comentarios']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=campos)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for prof in profesores:\n",
    "            url = construir_url(prof)\n",
    "            print(f\"Obteniendo datos de: {url}\")\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                rese√±as = extraer_rese√±as(response.text)\n",
    "                comentarios_texto = \", \".join(rese√±as)\n",
    "            except Exception as e:\n",
    "                print(f\"Error al obtener rese√±as de {prof['n']} {prof['a']}: {e}\")\n",
    "                comentarios_texto = \"\"\n",
    "\n",
    "            nombre_completo = f\"{prof['n']} {prof['a']}\"\n",
    "            writer.writerow({\n",
    "                'Nombre Completo': nombre_completo,\n",
    "                'Departamento': prof['d'],\n",
    "                'Calificaci√≥n': prof['c'],\n",
    "                'Comentarios': comentarios_texto\n",
    "            })\n",
    "            time.sleep(1)\n",
    "\n",
    "    print(f\"\\nArchivo CSV generado en: {ruta_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 2. Procesamiento de pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "\n",
    "def procesar_pdf_carpetas(carpeta_pdf, salida_csv):     \n",
    "    \"\"\"     \n",
    "    Procesa todos los archivos PDF en una carpeta y extrae el texto l√≠nea por l√≠nea a un CSV.          \n",
    "    Args:         \n",
    "        carpeta_pdf (str): Ruta de la carpeta que contiene los PDFs         \n",
    "        salida_csv (str): Ruta del archivo CSV de salida     \n",
    "    \"\"\"          \n",
    "    logging.basicConfig(level=logging.INFO)     \n",
    "    logger = logging.getLogger(__name__)          \n",
    "    datos = []          \n",
    "    try:                  \n",
    "        if not os.path.exists(carpeta_pdf):             \n",
    "            raise FileNotFoundError(f\"La carpeta {carpeta_pdf} no existe\")                           \n",
    "        archivos_pdf = [f for f in os.listdir(carpeta_pdf) if f.lower().endswith('.pdf')]                  \n",
    "        if not archivos_pdf:             \n",
    "            logger.warning(f\"No se encontraron archivos PDF en {carpeta_pdf}\")             \n",
    "            return                  \n",
    "        logger.info(f\"Encontrados {len(archivos_pdf)} archivos PDF\")                  \n",
    "        for archivo in archivos_pdf:             \n",
    "            ruta_pdf = os.path.join(carpeta_pdf, archivo)             \n",
    "            logger.info(f\"Procesando: {archivo}\")                          \n",
    "            try:                 \n",
    "                with pdfplumber.open(ruta_pdf) as pdf:                     \n",
    "                    for num_pagina, pagina in enumerate(pdf.pages, 1):                         \n",
    "                        try:                             \n",
    "                            texto = pagina.extract_text()                             \n",
    "                            if texto:                                 \n",
    "                                lineas = texto.split('\\n')                                 \n",
    "                                for linea in lineas:                                     \n",
    "                                    linea_limpia = linea.strip()                                     \n",
    "                                    if linea_limpia:                                           \n",
    "                                        datos.append({                                             \n",
    "                                            'Archivo': archivo,                                             \n",
    "                                            'Pagina': num_pagina,                                             \n",
    "                                            'Linea': linea_limpia                                         \n",
    "                                        })                         \n",
    "                        except Exception as e:                             \n",
    "                            logger.error(f\"Error procesando p√°gina {num_pagina} de {archivo}: {e}\")                             \n",
    "                            continue                                              \n",
    "                logger.info(f\"‚úì Completado: {archivo}\")                              \n",
    "            except Exception as e:                 \n",
    "                logger.error(f\"Error procesando {archivo}: {e}\")                 \n",
    "                continue                           \n",
    "        if datos:             \n",
    "            df = pd.DataFrame(datos)                                       \n",
    "            Path(salida_csv).parent.mkdir(parents=True, exist_ok=True)                          \n",
    "            df.to_csv(salida_csv, index=False, encoding='utf-8')             \n",
    "            logger.info(f\"‚úì Guardado exitosamente en {salida_csv}\")             \n",
    "            logger.info(f\"Total de l√≠neas extra√≠das: {len(datos)}\")         \n",
    "        else:             \n",
    "            logger.warning(\"No se extrajeron datos de ning√∫n archivo\")                  \n",
    "    except Exception as e:         \n",
    "        logger.error(f\"Error general: {e}\")         \n",
    "        raise  \n",
    "\n",
    "def procesar_pdf_individual(ruta_pdf, salida_csv):     \n",
    "    \"\"\"     \n",
    "    Procesa un solo archivo PDF.          \n",
    "    Args:         \n",
    "        ruta_pdf (str): Ruta del archivo PDF         \n",
    "        salida_csv (str): Ruta del archivo CSV de salida     \n",
    "    \"\"\"     \n",
    "    carpeta_pdf = os.path.dirname(ruta_pdf)     \n",
    "    archivo_pdf = os.path.basename(ruta_pdf)               \n",
    "    procesar_pdf_carpetas(carpeta_pdf, salida_csv)   \n",
    "\n",
    "if __name__ == \"__main__\":          \n",
    "    carpeta_pdf = \"# === NOTE: Replace with local path ===\"     \n",
    "    salida_csv = \"# === NOTE: Replace with local path ===\"               \n",
    "    procesar_pdf_carpetas(carpeta_pdf, salida_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 3. Agrupaci√≥n de la informaci√≥n extra√≠da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n para mostrar m√°s columnas y filas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    \"\"\"Limpia y normaliza texto\"\"\"\n",
    "    if pd.isna(texto):\n",
    "        return \"\"\n",
    "    texto = str(texto).strip()\n",
    "    # Eliminar caracteres especiales al inicio y final\n",
    "    texto = re.sub(r'^[‚Ä¢¬∑\\-\\*\\s]+', '', texto)\n",
    "    texto = re.sub(r'[‚Ä¢¬∑\\-\\*\\s]+$', '', texto)\n",
    "    return texto.strip()\n",
    "\n",
    "def es_nombre_profesor(texto):\n",
    "    \"\"\"\n",
    "    Identifica si un texto corresponde a un nombre de profesor\n",
    "    usando m√∫ltiples patrones y heur√≠sticas\n",
    "    \"\"\"\n",
    "    if pd.isna(texto) or texto.strip() == \"\":\n",
    "        return False\n",
    "    \n",
    "    texto = str(texto).strip()\n",
    "    \n",
    "    # Patr√≥n principal: APELLIDO APELLIDO, NOMBRE o variaciones\n",
    "    patron_nombre_formal = r'^[A-Z√Å√â√ç√ì√ö√ë][A-Z√Å√â√ç√ì√ö√ë\\s]+,\\s*[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±\\s]+$'\n",
    "    \n",
    "    # Patrones adicionales para nombres\n",
    "    patrones_nombre = [\n",
    "        r'^[A-Z][A-Z\\s]+,\\s*[A-Z][A-Z\\s]+$',  # TODO MAY√öSCULAS\n",
    "        r'^[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±A-Z√Å√â√ç√ì√ö√ë\\s]+,\\s*[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±A-Z√Å√â√ç√ì√ö√ë\\s]+$',  # Mixto\n",
    "        r'^[A-Z]{2,}\\s+[A-Z]{2,},\\s*[A-Z]{2,}',  # Apellidos cortos may√∫sculas\n",
    "    ]\n",
    "    \n",
    "    # Verificar patrones de nombre\n",
    "    for patron in [patron_nombre_formal] + patrones_nombre:\n",
    "        if re.match(patron, texto):\n",
    "            return True\n",
    "    \n",
    "    # Heur√≠sticas adicionales\n",
    "    # Si contiene coma y palabras en may√∫scula\n",
    "    if ',' in texto and len([word for word in texto.split() if word.isupper() and len(word) > 2]) >= 2:\n",
    "        return True\n",
    "    \n",
    "    # Si es todo may√∫sculas y tiene formato de nombre\n",
    "    if texto.isupper() and ',' in texto and len(texto.split()) >= 3:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def es_materia_o_codigo(texto):\n",
    "    \"\"\"Identifica si un texto corresponde a una materia o c√≥digo\"\"\"\n",
    "    if pd.isna(texto):\n",
    "        return False\n",
    "    \n",
    "    texto = str(texto).strip()\n",
    "    \n",
    "    # Patrones para materias\n",
    "    patrones_materia = [\n",
    "        r'.*\\([A-Z]\\d+\\)',  # Texto con c√≥digo entre par√©ntesis\n",
    "        r'^[A-Z]\\d+$',      # Solo c√≥digo\n",
    "        r'Semestre\\s+\\d+',   # Semestre X\n",
    "        r'^\\d+\\s*$',         # Solo n√∫meros\n",
    "        r'^[IVX]+\\s*$',      # N√∫meros romanos\n",
    "    ]\n",
    "    \n",
    "    for patron in patrones_materia:\n",
    "        if re.match(patron, texto):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def es_contenido_irrelevante(texto):\n",
    "    \"\"\"Identifica contenido que no es rese√±a ni nombre\"\"\"\n",
    "    if pd.isna(texto):\n",
    "        return True\n",
    "    \n",
    "    texto = str(texto).strip()\n",
    "    \n",
    "    if texto == \"\" or texto == \"‚Ä¢\" or texto == \"-\":\n",
    "        return True\n",
    "    \n",
    "    # Patrones de contenido irrelevante\n",
    "    patrones_irrelevantes = [\n",
    "        r'^\\d+\\s*$',  # Solo n√∫meros\n",
    "        r'^\\.\\.\\.*$',  # Solo puntos\n",
    "        r'^[‚Ä¢\\-\\*\\s]*$',  # Solo s√≠mbolos de lista\n",
    "        r'Semestre\\s+\\d+',\n",
    "        r'^\\d+\\s*\\.\\s*$',  # Numeraci√≥n\n",
    "        r'^Semestre',\n",
    "        r'^\\s*$'  # Solo espacios\n",
    "    ]\n",
    "    \n",
    "    for patron in patrones_irrelevantes:\n",
    "        if re.match(patron, texto):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def es_resena_valida(texto):\n",
    "    \"\"\"Valida si un texto es una rese√±a v√°lida\"\"\"\n",
    "    if pd.isna(texto) or texto.strip() == \"\":\n",
    "        return False\n",
    "    \n",
    "    texto = str(texto).strip()\n",
    "    \n",
    "    # Eliminar s√≠mbolos de lista\n",
    "    texto_limpio = re.sub(r'^[‚Ä¢¬∑\\-\\*\\s]+', '', texto)\n",
    "    \n",
    "    if len(texto_limpio) < 3:  # Muy corto\n",
    "        return False\n",
    "    \n",
    "    if es_nombre_profesor(texto) or es_materia_o_codigo(texto) or es_contenido_irrelevante(texto):\n",
    "        return False\n",
    "    \n",
    "    # Debe tener contenido significativo\n",
    "    palabras = texto_limpio.split()\n",
    "    if len(palabras) < 2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def procesar_csv_resenas(filepath):\n",
    "    \"\"\"Procesa el CSV y agrupa rese√±as por profesor\"\"\"\n",
    "    \n",
    "    print(\"Cargando archivo CSV...\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, encoding='latin-1')\n",
    "        except:\n",
    "            df = pd.read_csv(filepath, encoding='cp1252')\n",
    "    \n",
    "    print(f\"Archivo cargado. Total de filas: {len(df)}\")\n",
    "    print(f\"Columnas: {list(df.columns)}\")\n",
    "    \n",
    "    # Renombrar columnas para consistencia\n",
    "    if len(df.columns) >= 3:\n",
    "        df.columns = ['Archivo', 'Pagina', 'Texto']\n",
    "    \n",
    "    # Limpiar texto\n",
    "    df['Texto_Limpio'] = df['Texto'].apply(limpiar_texto)\n",
    "    \n",
    "    # Filtrar filas vac√≠as\n",
    "    df = df[df['Texto_Limpio'] != \"\"]\n",
    "    \n",
    "    print(f\"Despu√©s de limpiar: {len(df)} filas\")\n",
    "    \n",
    "    # Crear estructura para almacenar resultados\n",
    "    resultados = []\n",
    "    \n",
    "    # Variables de estado\n",
    "    profesor_actual = None\n",
    "    materia_actual = None\n",
    "    resenas_actuales = []\n",
    "    \n",
    "    print(\"Procesando filas...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        texto = row['Texto_Limpio']\n",
    "        archivo = row['Archivo']\n",
    "        pagina = row['Pagina']\n",
    "        \n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Procesando fila {idx}/{len(df)}...\")\n",
    "        \n",
    "        # Verificar tipo de contenido\n",
    "        if es_materia_o_codigo(texto):\n",
    "            # Guardar rese√±as anteriores si existen\n",
    "            if profesor_actual and resenas_actuales:\n",
    "                resena_completa = ' '.join(resenas_actuales)\n",
    "                resultados.append({\n",
    "                    'Archivo': archivo,\n",
    "                    'Pagina': pagina,\n",
    "                    'Profesor': profesor_actual,\n",
    "                    'Materia': materia_actual,\n",
    "                    'Resena': resena_completa,\n",
    "                    'Num_Fragmentos': len(resenas_actuales)\n",
    "                })\n",
    "            \n",
    "            # Nueva materia\n",
    "            materia_actual = texto\n",
    "            profesor_actual = None\n",
    "            resenas_actuales = []\n",
    "            \n",
    "        elif es_nombre_profesor(texto):\n",
    "            # Guardar rese√±as del profesor anterior\n",
    "            if profesor_actual and resenas_actuales:\n",
    "                resena_completa = ' '.join(resenas_actuales)\n",
    "                resultados.append({\n",
    "                    'Archivo': archivo,\n",
    "                    'Pagina': pagina,\n",
    "                    'Profesor': profesor_actual,\n",
    "                    'Materia': materia_actual,\n",
    "                    'Resena': resena_completa,\n",
    "                    'Num_Fragmentos': len(resenas_actuales)\n",
    "                })\n",
    "            \n",
    "            # Nuevo profesor\n",
    "            profesor_actual = texto\n",
    "            resenas_actuales = []\n",
    "            \n",
    "        elif es_resena_valida(texto):\n",
    "            # Agregar rese√±a al profesor actual\n",
    "            if profesor_actual:\n",
    "                # Limpiar s√≠mbolos de lista\n",
    "                texto_resena = re.sub(r'^[‚Ä¢¬∑\\-\\*\\s]+', '', texto)\n",
    "                if texto_resena:\n",
    "                    resenas_actuales.append(texto_resena)\n",
    "        \n",
    "        elif es_contenido_irrelevante(texto):\n",
    "            # Ignorar contenido irrelevante\n",
    "            continue\n",
    "    \n",
    "    # Procesar √∫ltima rese√±a\n",
    "    if profesor_actual and resenas_actuales:\n",
    "        resena_completa = ' '.join(resenas_actuales)\n",
    "        resultados.append({\n",
    "            'Archivo': df.iloc[-1]['Archivo'],\n",
    "            'Pagina': df.iloc[-1]['Pagina'],\n",
    "            'Profesor': profesor_actual,\n",
    "            'Materia': materia_actual,\n",
    "            'Resena': resena_completa,\n",
    "            'Num_Fragmentos': len(resenas_actuales)\n",
    "        })\n",
    "    \n",
    "    # Crear DataFrame con resultados\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    \n",
    "    print(f\"\\nProcesamiento completado!\")\n",
    "    print(f\"Total de rese√±as agrupadas: {len(df_resultados)}\")\n",
    "    \n",
    "    return df_resultados, df\n",
    "\n",
    "def analizar_resultados(df_resultados):\n",
    "    \"\"\"Analiza los resultados obtenidos\"\"\"\n",
    "    print(\"\\n=== AN√ÅLISIS DE RESULTADOS ===\")\n",
    "    print(f\"Total de rese√±as procesadas: {len(df_resultados)}\")\n",
    "    \n",
    "    if len(df_resultados) > 0:\n",
    "        print(f\"Profesores √∫nicos: {df_resultados['Profesor'].nunique()}\")\n",
    "        print(f\"Materias √∫nicas: {df_resultados['Materia'].nunique()}\")\n",
    "        print(f\"Archivos procesados: {df_resultados['Archivo'].nunique()}\")\n",
    "        \n",
    "        # Estad√≠sticas de fragmentos\n",
    "        fragmentos_stats = df_resultados['Num_Fragmentos'].describe()\n",
    "        print(f\"\\nEstad√≠sticas de fragmentos por rese√±a:\")\n",
    "        print(fragmentos_stats)\n",
    "        \n",
    "        # Top profesores con m√°s rese√±as\n",
    "        print(f\"\\nTop 10 profesores con m√°s rese√±as:\")\n",
    "        top_profesores = df_resultados['Profesor'].value_counts().head(10)\n",
    "        print(top_profesores)\n",
    "        \n",
    "        # Ejemplos de rese√±as\n",
    "        print(f\"\\n=== EJEMPLOS DE RESE√ëAS AGRUPADAS ===\")\n",
    "        for idx, row in df_resultados.head(5).iterrows():\n",
    "            print(f\"\\nProfesor: {row['Profesor']}\")\n",
    "            print(f\"Materia: {row['Materia']}\")\n",
    "            print(f\"Rese√±a ({row['Num_Fragmentos']} fragmentos): {row['Resena'][:200]}...\")\n",
    "    \n",
    "    return df_resultados\n",
    "\n",
    "def guardar_resultados(df_resultados, output_path=None):\n",
    "    \"\"\"Guarda los resultados en CSV\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = r\"# === NOTE: Replace with local path ===\"\n",
    "    \n",
    "    df_resultados.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"\\nResultados guardados en: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Funci√≥n principal\n",
    "def main():\n",
    "    # Ruta del archivo\n",
    "    filepath = r\"# === NOTE: Replace with local path ===\"\n",
    "    \n",
    "    try:\n",
    "        # Procesar archivo\n",
    "        df_resultados, df_original = procesar_csv_resenas(filepath)\n",
    "        \n",
    "        # Analizar resultados\n",
    "        df_resultados = analizar_resultados(df_resultados)\n",
    "        \n",
    "        # Guardar resultados\n",
    "        output_path = guardar_resultados(df_resultados)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Proceso completado exitosamente!\")\n",
    "        print(f\"üìÅ Archivo original: {len(df_original)} filas\")\n",
    "        print(f\"üìä Rese√±as agrupadas: {len(df_resultados)} rese√±as\")\n",
    "        print(f\"üíæ Guardado en: {output_path}\")\n",
    "        \n",
    "        return df_resultados, df_original\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error durante el procesamiento: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_agrupado, df_raw = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 4. Unificaci√≥n de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import unicodedata\n",
    "\n",
    "class UnificadorDatasetsProfesores:\n",
    "    def __init__(self):\n",
    "        self.threshold_nombres = 85  # Umbral para matching de nombres\n",
    "        \n",
    "    def limpiar_nombre(self, nombre):\n",
    "        \"\"\"Limpia y normaliza nombres\"\"\"\n",
    "        if pd.isna(nombre):\n",
    "            return \"\"\n",
    "        \n",
    "        nombre = str(nombre).upper().strip()\n",
    "        # Remover acentos\n",
    "        nombre = unicodedata.normalize('NFD', nombre)\n",
    "        nombre = ''.join(c for c in nombre if unicodedata.category(c) != 'Mn')\n",
    "        \n",
    "        return nombre\n",
    "    \n",
    "    def extraer_nombres_apellidos(self, nombre):\n",
    "        \"\"\"Extrae nombres y apellidos en formato est√°ndar\"\"\"\n",
    "        nombre_limpio = self.limpiar_nombre(nombre)\n",
    "        \n",
    "        # Patr√≥n: \"APELLIDO APELLIDO, NOMBRE NOMBRE\"\n",
    "        if ',' in nombre_limpio:\n",
    "            partes = nombre_limpio.split(',', 1)\n",
    "            apellidos = partes[0].strip()\n",
    "            nombres = partes[1].strip()\n",
    "        else:\n",
    "            # Sin coma, separar palabras\n",
    "            palabras = nombre_limpio.split()\n",
    "            if len(palabras) == 1:\n",
    "                apellidos = palabras[0]\n",
    "                nombres = \"\"\n",
    "            elif len(palabras) == 2:\n",
    "                nombres = palabras[0]\n",
    "                apellidos = palabras[1]\n",
    "            else:\n",
    "                # Asumir √∫ltimas 2 palabras como apellidos\n",
    "                apellidos = ' '.join(palabras[-2:])\n",
    "                nombres = ' '.join(palabras[:-2])\n",
    "        \n",
    "        return apellidos.strip(), nombres.strip()\n",
    "    \n",
    "    def crear_variaciones_nombre(self, nombre):\n",
    "        \"\"\"Crea variaciones posibles de un nombre\"\"\"\n",
    "        apellidos, nombres = self.extraer_nombres_apellidos(nombre)\n",
    "        \n",
    "        variaciones = [\n",
    "            f\"{apellidos}, {nombres}\",  # Formato completo\n",
    "            f\"{apellidos}\",  # Solo apellidos\n",
    "            f\"{nombres} {apellidos}\",  # Orden normal\n",
    "            apellidos.split()[0] if apellidos else \"\",  # Primer apellido\n",
    "        ]\n",
    "        \n",
    "        return [v for v in variaciones if v.strip()]\n",
    "    \n",
    "    def encontrar_mejor_match(self, nombre_buscar, lista_nombres):\n",
    "        \"\"\"Encuentra el mejor match usando fuzzy matching\"\"\"\n",
    "        variaciones = self.crear_variaciones_nombre(nombre_buscar)\n",
    "        mejor_match = None\n",
    "        mejor_score = 0\n",
    "        \n",
    "        for variacion in variaciones:\n",
    "            if not variacion:\n",
    "                continue\n",
    "                \n",
    "            for nombre_candidato in lista_nombres:\n",
    "                variaciones_candidato = self.crear_variaciones_nombre(nombre_candidato)\n",
    "                \n",
    "                for var_candidato in variaciones_candidato:\n",
    "                    score = fuzz.ratio(variacion, var_candidato)\n",
    "                    if score > mejor_score:\n",
    "                        mejor_score = score\n",
    "                        mejor_match = nombre_candidato\n",
    "        \n",
    "        return mejor_match if mejor_score >= self.threshold_nombres else None, mejor_score\n",
    "    \n",
    "    def cargar_datasets(self, ruta_dataset1, ruta_dataset2):\n",
    "        \"\"\"Carga ambos datasets\"\"\"\n",
    "        \n",
    "        df1 = pd.read_csv(ruta_dataset1)\n",
    "        print(f\"Dataset 1 cargado: {len(df1)} filas\")\n",
    "        print(\"Columnas:\", df1.columns.tolist())\n",
    "        \n",
    "         \n",
    "        df2 = pd.read_csv(ruta_dataset2)\n",
    "        print(f\"Dataset 2 cargado: {len(df2)} filas\")\n",
    "        print(\"Columnas:\", df2.columns.tolist())\n",
    "        \n",
    "        return df1, df2\n",
    "    \n",
    "    def unificar_datasets(self, ruta_dataset1, ruta_dataset2):\n",
    "        \"\"\"Funci√≥n principal que unifica ambos datasets\"\"\"\n",
    "        \n",
    "        # Cargar datasets\n",
    "        df1, df2 = self.cargar_datasets(ruta_dataset1, ruta_dataset2)\n",
    "        \n",
    "        # Preparar dataset 1 \n",
    "        df1_procesado = df1.copy()\n",
    "        df1_procesado['nombre_normalizado'] = df1_procesado['Nombre Completo'].apply(self.limpiar_nombre)\n",
    "        df1_procesado['fuente'] = 'dataset1'\n",
    "        \n",
    "        # Preparar dataset 2 \n",
    "        df2_procesado = df2.copy()\n",
    "        df2_procesado['nombre_normalizado'] = df2_procesado['Profesor'].apply(self.limpiar_nombre)\n",
    "        df2_procesado['fuente'] = 'dataset2'\n",
    "        \n",
    "        # Crear estructura unificada\n",
    "        dataset_unificado = []\n",
    "        matches_encontrados = {}\n",
    "        \n",
    "        # Procesar dataset 1\n",
    "        print(\"\\nProcesando Dataset 1...\")\n",
    "        for idx, row in df1_procesado.iterrows():\n",
    "            registro = {\n",
    "                'id': f\"d1_{idx}\",\n",
    "                'nombre_original': row['Nombre Completo'],\n",
    "                'nombre_normalizado': row['nombre_normalizado'],\n",
    "                'departamento': row.get('Departamento', ''),\n",
    "                'materia': '',  # No tiene materia espec√≠fica\n",
    "                'calificacion_numerica': row.get('Calificaci√≥n', None),\n",
    "                'comentarios': row.get('Comentarios', ''),\n",
    "                'resena_detallada': '',\n",
    "                'archivo_fuente': '',\n",
    "                'pagina': None,\n",
    "                'num_fragmentos': None,\n",
    "                'fuente': 'dataset1',\n",
    "                'match_id': None\n",
    "            }\n",
    "            dataset_unificado.append(registro)\n",
    "        \n",
    "        # Procesar dataset 2 y buscar matches\n",
    "        print(\"Procesando Dataset 2 y buscando matches...\")\n",
    "        nombres_d1 = df1_procesado['Nombre Completo'].tolist()\n",
    "        \n",
    "        for idx, row in df2_procesado.iterrows():\n",
    "            # Buscar match en dataset 1\n",
    "            mejor_match, score = self.encontrar_mejor_match(\n",
    "                row['Profesor'], nombres_d1\n",
    "            )\n",
    "            \n",
    "            registro = {\n",
    "                'id': f\"d2_{idx}\",\n",
    "                'nombre_original': row['Profesor'],\n",
    "                'nombre_normalizado': row['nombre_normalizado'],\n",
    "                'departamento': '',  # Se puede inferir de la materia\n",
    "                'materia': row.get('Materia', ''),\n",
    "                'calificacion_numerica': None,\n",
    "                'comentarios': '',\n",
    "                'resena_detallada': row.get('Resena', ''),\n",
    "                'archivo_fuente': row.get('Archivo', ''),\n",
    "                'pagina': row.get('Pagina', None),\n",
    "                'num_fragmentos': row.get('Num_Fragmentos', None),\n",
    "                'fuente': 'dataset2',\n",
    "                'match_id': mejor_match if score >= self.threshold_nombres else None,\n",
    "                'match_score': score\n",
    "            }\n",
    "            \n",
    "            if mejor_match:\n",
    "                if mejor_match not in matches_encontrados:\n",
    "                    matches_encontrados[mejor_match] = []\n",
    "                matches_encontrados[mejor_match].append(registro['id'])\n",
    "                print(f\"Match encontrado: {row['Profesor']} -> {mejor_match} (Score: {score})\")\n",
    "            \n",
    "            dataset_unificado.append(registro)\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        df_unificado = pd.DataFrame(dataset_unificado)\n",
    "        \n",
    "        # Estad√≠sticas\n",
    "        print(f\"\\n=== ESTAD√çSTICAS DE UNIFICACI√ìN ===\")\n",
    "        print(f\"Total registros unificados: {len(df_unificado)}\")\n",
    "        print(f\"Del Dataset 1: {len(df_unificado[df_unificado['fuente'] == 'dataset1'])}\")\n",
    "        print(f\"Del Dataset 2: {len(df_unificado[df_unificado['fuente'] == 'dataset2'])}\")\n",
    "        print(f\"Matches encontrados: {len(matches_encontrados)}\")\n",
    "        print(f\"Profesores √∫nicos (aproximado): {df_unificado['nombre_normalizado'].nunique()}\")\n",
    "        \n",
    "        return df_unificado, matches_encontrados\n",
    "    \n",
    "    def consolidar_por_profesor(self, df_unificado):\n",
    "        \"\"\"Consolida registros del mismo profesor\"\"\"\n",
    "        \n",
    "        profesores_consolidados = []\n",
    "        \n",
    "        # Agrupar por nombre normalizado\n",
    "        grupos = df_unificado.groupby('nombre_normalizado')\n",
    "        \n",
    "        for nombre_norm, grupo in grupos:\n",
    "            # Tomar el primer registro como base\n",
    "            registro_base = grupo.iloc[0].copy()\n",
    "            \n",
    "            # Combinar informaci√≥n de ambos datasets\n",
    "            comentarios_d1 = []\n",
    "            resenas_d2 = []\n",
    "            materias = []\n",
    "            calificaciones = []\n",
    "            archivos = []\n",
    "            \n",
    "            for _, row in grupo.iterrows():\n",
    "                if row['fuente'] == 'dataset1':\n",
    "                    if row['comentarios']:\n",
    "                        comentarios_d1.append(row['comentarios'])\n",
    "                    if pd.notna(row['calificacion_numerica']):\n",
    "                        calificaciones.append(row['calificacion_numerica'])\n",
    "                        \n",
    "                elif row['fuente'] == 'dataset2':\n",
    "                    if row['resena_detallada']:\n",
    "                        resenas_d2.append(row['resena_detallada'])\n",
    "                    if row['materia']:\n",
    "                        materias.append(row['materia'])\n",
    "                    if row['archivo_fuente']:\n",
    "                        archivos.append(row['archivo_fuente'])\n",
    "            \n",
    "            # Crear registro consolidado\n",
    "            profesor_consolidado = {\n",
    "                'nombre': registro_base['nombre_original'],\n",
    "                'nombre_normalizado': nombre_norm,\n",
    "                'departamento': registro_base['departamento'],\n",
    "                'materias': list(set(materias)),  # Materias √∫nicas\n",
    "                'calificacion_promedio': np.mean(calificaciones) if calificaciones else None,\n",
    "                'num_calificaciones': len(calificaciones),\n",
    "                'comentarios_estructurados': comentarios_d1,\n",
    "                'resenas_detalladas': resenas_d2,\n",
    "                'total_comentarios': len(comentarios_d1) + len(resenas_d2),\n",
    "                'archivos_fuente': list(set(archivos)),\n",
    "                'datasets_presente': list(grupo['fuente'].unique())\n",
    "            }\n",
    "            \n",
    "            profesores_consolidados.append(profesor_consolidado)\n",
    "        \n",
    "        return pd.DataFrame(profesores_consolidados)\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    unificador = UnificadorDatasetsProfesores()\n",
    "    \n",
    "    # Rutas de tus archivos\n",
    "    ruta_dataset1 = r\"# === NOTE: Replace with local path ===\"\n",
    "    ruta_dataset2 = r\"# === NOTE: Replace with local path ===\"\n",
    "    \n",
    "    # Unificar datasets\n",
    "    df_unificado, matches = unificador.unificar_datasets(ruta_dataset1, ruta_dataset2)\n",
    "    \n",
    "    # Guardar resultado intermedio\n",
    "    df_unificado.to_csv(\"dataset_unificado_raw.csv\", index=False)\n",
    "    print(\"\\nDataset unificado guardado como 'dataset_unificado_raw.csv'\")\n",
    "    \n",
    "    # Consolidar por profesor\n",
    "    df_consolidado = unificador.consolidar_por_profesor(df_unificado)\n",
    "    \n",
    "    # Guardar resultado final\n",
    "    df_consolidado.to_csv(\"profesores_consolidados.csv\", index=False)\n",
    "    print(\"Dataset consolidado guardado como 'profesores_consolidados.csv'\")\n",
    "    \n",
    "    # Mostrar algunos ejemplos\n",
    "    print(\"\\n=== EJEMPLOS DE PROFESORES CONSOLIDADOS ===\")\n",
    "    for idx, profesor in df_consolidado.head(3).iterrows():\n",
    "        print(f\"\\nProfesor: {profesor['nombre']}\")\n",
    "        print(f\"Materias: {profesor['materias']}\")\n",
    "        print(f\"Calificaci√≥n promedio: {profesor['calificacion_promedio']}\")\n",
    "        print(f\"Total comentarios: {profesor['total_comentarios']}\")\n",
    "        print(f\"Presente en datasets: {profesor['datasets_presente']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

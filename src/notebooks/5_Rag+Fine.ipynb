{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\morde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "======================================================================\n",
      "FINE-TUNING H√çBRIDO PHI-3-MINI\n",
      "======================================================================\n",
      "\n",
      "Cargando datasets...\n",
      "‚úÖ Train: 1109 ejemplos\n",
      "‚úÖ Val: 196 ejemplos\n",
      "Cargando Phi-3-Mini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd091554364e42ecad9876bfef67f79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo cargado\n",
      "VRAM usada: 2.11GB\n",
      "\n",
      "Preparando LoRA...\n",
      "‚úÖ LoRA configurado\n",
      "trainable params: 4,456,448 || all params: 3,825,536,000 || trainable%: 0.1165\n",
      "\n",
      "Configurando entrenamiento...\n",
      "‚úÖ Training args configurados\n",
      "\n",
      "Creando SFTTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\morde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\morde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20eec66c638543ceb4ac79a6c1e9fcdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\morde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer creado\n",
      "VRAM total: 2.49GB\n",
      "\n",
      "======================================================================\n",
      "INICIANDO ENTRENAMIENTO\n",
      "======================================================================\n",
      "√âpocas: 3\n",
      "Batch efectivo: 16\n",
      "Learning rate: 3e-05\n",
      "Early stopping patience: 2\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ae2ef09c5d499b87750362b9430808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 18.4013, 'grad_norm': 3.1231167316436768, 'learning_rate': 4.2857142857142855e-06, 'epoch': 0.01}\n",
      "{'loss': 18.909, 'grad_norm': 4.985228061676025, 'learning_rate': 2.9983348124429553e-05, 'epoch': 0.14}\n",
      "{'loss': 18.1195, 'grad_norm': 5.14055871963501, 'learning_rate': 2.9688342159326487e-05, 'epoch': 0.29}\n",
      "{'loss': 16.8247, 'grad_norm': 5.305100440979004, 'learning_rate': 2.903166046244801e-05, 'epoch': 0.43}\n",
      "{'loss': 15.5561, 'grad_norm': 5.1938018798828125, 'learning_rate': 2.8029472716572872e-05, 'epoch': 0.58}\n",
      "{'loss': 14.3555, 'grad_norm': 6.346128463745117, 'learning_rate': 2.6706456110074946e-05, 'epoch': 0.72}\n",
      "{'loss': 13.2789, 'grad_norm': 6.525103569030762, 'learning_rate': 2.50951877026466e-05, 'epoch': 0.86}\n",
      "{'loss': 11.7022, 'grad_norm': 7.763555526733398, 'learning_rate': 2.3235342269971978e-05, 'epoch': 1.01}\n",
      "{'loss': 10.2984, 'grad_norm': 11.638710975646973, 'learning_rate': 2.1172715379076635e-05, 'epoch': 1.15}\n",
      "{'loss': 9.4233, 'grad_norm': 6.23150110244751, 'learning_rate': 1.8958095749480597e-05, 'epoch': 1.3}\n",
      "{'loss': 8.3963, 'grad_norm': 5.279521465301514, 'learning_rate': 1.6646014666365676e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f75fd048a824378ac730b4aeba9f242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0598777532577515, 'eval_runtime': 89.3787, 'eval_samples_per_second': 2.193, 'eval_steps_per_second': 1.096, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\morde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/Phi-3-mini-4k-instruct/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000017B11040EE0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 34192a1f-cc26-43a4-b61f-346952058f6d)') - silently ignoring the lookup for the file config.json in microsoft/Phi-3-mini-4k-instruct.\n",
      "  warnings.warn(\n",
      "c:\\Users\\morde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in microsoft/Phi-3-mini-4k-instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.3283, 'grad_norm': 3.3616693019866943, 'learning_rate': 1.4293403239355362e-05, 'epoch': 1.59}\n",
      "{'loss': 8.0329, 'grad_norm': 3.8594794273376465, 'learning_rate': 1.1958190569652318e-05, 'epoch': 1.73}\n",
      "{'loss': 7.8571, 'grad_norm': 3.680961847305298, 'learning_rate': 9.697877343311145e-06, 'epoch': 1.87}\n",
      "{'loss': 7.6637, 'grad_norm': 3.1424615383148193, 'learning_rate': 7.568119973513886e-06, 'epoch': 2.02}\n",
      "{'loss': 7.2101, 'grad_norm': 3.3247694969177246, 'learning_rate': 5.621360154964428e-06, 'epoch': 2.16}\n",
      "{'loss': 7.467, 'grad_norm': 3.82478404045105, 'learning_rate': 3.905533575320855e-06, 'epoch': 2.31}\n",
      "{'loss': 7.1354, 'grad_norm': 3.0702810287475586, 'learning_rate': 2.4628895794759493e-06, 'epoch': 2.45}\n",
      "{'loss': 7.1796, 'grad_norm': 3.184539556503296, 'learning_rate': 1.3289508504683206e-06, 'epoch': 2.59}\n",
      "{'loss': 7.1519, 'grad_norm': 3.030181646347046, 'learning_rate': 5.316387231330288e-07, 'epoch': 2.74}\n",
      "{'loss': 7.0042, 'grad_norm': 3.0833263397216797, 'learning_rate': 9.058566817230606e-08, 'epoch': 2.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30108859a1d4fb48b977dfc35b58a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8897932171821594, 'eval_runtime': 90.0382, 'eval_samples_per_second': 2.177, 'eval_steps_per_second': 1.088, 'epoch': 2.88}\n",
      "{'train_runtime': 4493.5151, 'train_samples_per_second': 0.74, 'train_steps_per_second': 0.046, 'train_loss': 10.473551542862602, 'epoch': 2.98}\n",
      "\n",
      "======================================================================\n",
      "ENTRENAMIENTO COMPLETADO\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Modelo guardado en: phi3_hybrid_model\\final_adapter\n",
      "VRAM final: 2.52GB\n",
      "\n",
      "üìä Estad√≠sticas:\n",
      "   √âpocas completadas: 3\n",
      "   Ejemplos train: 1109\n",
      "   Ejemplos val: 196\n",
      "\n",
      "üí° SIGUIENTE PASO:\n",
      "   Ejecuta inference_hybrid.py para probar el sistema completo\n",
      "\n",
      "‚úÖ Limpieza completada\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \n",
    "    TRAIN_FILE = \"./hybrid_training_data/train.jsonl\"\n",
    "    VAL_FILE = \"./hybrid_training_data/validation.jsonl\"\n",
    "    OUTPUT_DIR = \"./phi3_hybrid_model\"\n",
    "    \n",
    "    \n",
    "    MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "    \n",
    "    \n",
    "    NUM_EPOCHS = 3  \n",
    "    BATCH_SIZE = 2  \n",
    "    GRAD_ACCUM_STEPS = 8  \n",
    "    MAX_SEQ_LENGTH = 512\n",
    "    LEARNING_RATE = 3e-5  \n",
    "    \n",
    "    \n",
    "    LORA_R = 8 \n",
    "    LORA_ALPHA = 16\n",
    "    LORA_DROPOUT = 0.1\n",
    "    \n",
    "    \n",
    "    EARLY_STOPPING_PATIENCE = 2\n",
    "    \n",
    "    \n",
    "    GPU_MEMORY = \"4.5GB\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "def cleanup():\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def setup_model_and_tokenizer():\n",
    "    \n",
    "    print(\"Cargando Phi-3-Mini...\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.MODEL_NAME,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    print(\" Modelo cargado\")\n",
    "    print(f\"VRAM usada: {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def prepare_lora(model):\n",
    "    \"\"\"Configura LoRA\"\"\"\n",
    "    print(\"\\nPreparando LoRA...\")\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=config.LORA_R,\n",
    "        lora_alpha=config.LORA_ALPHA,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                       \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_dropout=config.LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    from peft import get_peft_model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(\" LoRA configurado\")\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def formatting_func(example):\n",
    "    \n",
    "    messages = example['messages']\n",
    "    \n",
    "    \n",
    "    if isinstance(messages[0], list):\n",
    "        \n",
    "        outputs = []\n",
    "        for msg_list in messages:\n",
    "            system_msg = \"\"\n",
    "            user_msg = \"\"\n",
    "            assistant_msg = \"\"\n",
    "            \n",
    "            for msg in msg_list:\n",
    "                if msg['role'] == 'system':\n",
    "                    system_msg = msg['content']\n",
    "                elif msg['role'] == 'user':\n",
    "                    user_msg = msg['content']\n",
    "                elif msg['role'] == 'assistant':\n",
    "                    assistant_msg = msg['content']\n",
    "            \n",
    "            text = f\"<|system|>\\n{system_msg}<|end|>\\n<|user|>\\n{user_msg}<|end|>\\n<|assistant|>\\n{assistant_msg}<|end|>\"\n",
    "            outputs.append(text)\n",
    "        \n",
    "        return outputs\n",
    "    else:\n",
    "        \n",
    "        system_msg = \"\"\n",
    "        user_msg = \"\"\n",
    "        assistant_msg = \"\"\n",
    "        \n",
    "        for msg in messages:\n",
    "            if msg['role'] == 'system':\n",
    "                system_msg = msg['content']\n",
    "            elif msg['role'] == 'user':\n",
    "                user_msg = msg['content']\n",
    "            elif msg['role'] == 'assistant':\n",
    "                assistant_msg = msg['content']\n",
    "        \n",
    "        text = f\"<|system|>\\n{system_msg}<|end|>\\n<|user|>\\n{user_msg}<|end|>\\n<|assistant|>\\n{assistant_msg}<|end|>\"\n",
    "        \n",
    "        return [text]\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"FINE-TUNING H√çBRIDO PHI-3-MINI\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    \n",
    "    cleanup()\n",
    "    \n",
    "   \n",
    "    if not Path(config.TRAIN_FILE).exists():\n",
    "        print(f\" No se encuentra: {config.TRAIN_FILE}\")\n",
    "        print(\"Ejecuta primero generate_hybrid_dataset.py\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    print(\"\\nCargando datasets...\")\n",
    "    train_dataset = load_dataset('json', data_files=config.TRAIN_FILE, split='train')\n",
    "    val_dataset = load_dataset('json', data_files=config.VAL_FILE, split='train')\n",
    "    \n",
    "    print(f\" Train: {len(train_dataset)} ejemplos\")\n",
    "    print(f\" Val: {len(val_dataset)} ejemplos\")\n",
    "    \n",
    "   \n",
    "    model, tokenizer = setup_model_and_tokenizer()\n",
    "    model = prepare_lora(model)\n",
    "    \n",
    "    \n",
    "    print(\"\\nConfigurando entrenamiento...\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=config.OUTPUT_DIR,\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRAD_ACCUM_STEPS,\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=0.3,\n",
    "        bf16=True,\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        \n",
    "        \n",
    "        logging_steps=10,\n",
    "        logging_dir=f\"{config.OUTPUT_DIR}/logs\",\n",
    "        logging_first_step=True,\n",
    "        \n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        \n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        \n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        \n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=True,  \n",
    "        \n",
    "        seed=42,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    print(\" Training args configurados\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nCreando SFTTrainer...\")\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        formatting_func=formatting_func,\n",
    "        max_seq_length=config.MAX_SEQ_LENGTH,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=config.EARLY_STOPPING_PATIENCE,\n",
    "                early_stopping_threshold=0.001\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\" Trainer creado\")\n",
    "    print(f\"VRAM total: {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB\\n\")\n",
    "    \n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"INICIANDO ENTRENAMIENTO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"√âpocas: {config.NUM_EPOCHS}\")\n",
    "    print(f\"Batch efectivo: {config.BATCH_SIZE * config.GRAD_ACCUM_STEPS}\")\n",
    "    print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
    "    print(f\"Early stopping patience: {config.EARLY_STOPPING_PATIENCE}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        trainer.train()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ENTRENAMIENTO COMPLETADO\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        \n",
    "        output_path = Path(config.OUTPUT_DIR) / \"final_adapter\"\n",
    "        trainer.model.save_pretrained(output_path)\n",
    "        tokenizer.save_pretrained(output_path)\n",
    "        \n",
    "        print(f\"\\n Modelo guardado en: {output_path}\")\n",
    "        print(f\"VRAM final: {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB\")\n",
    "        \n",
    "        \n",
    "        print(f\"\\n Estad√≠sticas:\")\n",
    "        print(f\"   √âpocas completadas: {config.NUM_EPOCHS}\")\n",
    "        print(f\"   Ejemplos train: {len(train_dataset)}\")\n",
    "        print(f\"   Ejemplos val: {len(val_dataset)}\")\n",
    "        \n",
    "        print(f\"\\nüí° SIGUIENTE PASO:\")\n",
    "        print(f\"   Ejecuta inference_hybrid.py para probar el sistema completo\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Entrenamiento interrumpido\")\n",
    "        print(\"Guardando checkpoint de emergencia...\")\n",
    "        trainer.save_model(f\"{config.OUTPUT_DIR}/emergency_checkpoint\")\n",
    "        print(\" Checkpoint guardado\")\n",
    "    \n",
    "    finally:\n",
    "        cleanup()\n",
    "        print(\"\\n Limpieza completada\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
